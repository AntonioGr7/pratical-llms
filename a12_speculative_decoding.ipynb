{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speculative Decoding\n",
    "[Explain speculative decoding here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this if you haven't already done it\n",
    "```\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
    "!pip install -r llama.cpp/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Can we use a smaller (and faster) quantized model as speculative model for the bigger one?\n",
    "Let's see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the bigger model here\n",
    "Let's test using the newest LLama3-70B-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-19 17:17:30--  https://huggingface.co/NousResearch/Meta-Llama-3-70B-Instruct-GGUF/resolve/main/Meta-Llama-3-70B-Instruct-Q3_K_S.gguf\n",
      "Resolving huggingface.co (huggingface.co)... 108.138.189.74, 108.138.189.57, 108.138.189.70, ...\n",
      "Connecting to huggingface.co (huggingface.co)|108.138.189.74|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/2a/b0/2ab04cb3294326d82544e8b8ccdd51bdcf0b3e243e3f715a528f2fbaae0d8f47/8e6224569b0c43c15b0f75d4e03bbce38e856de623758c332d8972e9bbf9163b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Meta-Llama-3-70B-Instruct-Q3_K_S.gguf%3B+filename%3D%22Meta-Llama-3-70B-Instruct-Q3_K_S.gguf%22%3B&Expires=1713799050&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzc5OTA1MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzJhL2IwLzJhYjA0Y2IzMjk0MzI2ZDgyNTQ0ZThiOGNjZGQ1MWJkY2YwYjNlMjQzZTNmNzE1YTUyOGYyZmJhYWUwZDhmNDcvOGU2MjI0NTY5YjBjNDNjMTViMGY3NWQ0ZTAzYmJjZTM4ZTg1NmRlNjIzNzU4YzMzMmQ4OTcyZTliYmY5MTYzYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=PhE6oes-v6cAww2DjUxO-mXL5L%7EndBFEJkgrjWx4uAdi06ANXID27CWVV5eDKcycBERabbNzChQB4UtKCZkWbW0BcDV5ruyBjCFLLvChJ-SpZ2EcFORY5dRoBUTVwsW0HE5ZNMPGqNl3LHM2TSkyBnrGg8lKxjNLzyGWKlp0JNZPT53xdw2KlleqreeYLX4ISLvgnum0GTmsvvY%7EvYnX9uIX83fD%7E80cqSI0ekjyd5p8CpCmp6iBYBy%7E-RdeFN0B-kuTfPqQjFKbgshGLMZD7r2Ge5KuaynAA34iErk%7EcOAUP1tf5AsWNjVvXQEZdlFWPPKzXx01UHM8d--d1v0hpA__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
      "--2024-04-19 17:17:31--  https://cdn-lfs-us-1.huggingface.co/repos/2a/b0/2ab04cb3294326d82544e8b8ccdd51bdcf0b3e243e3f715a528f2fbaae0d8f47/8e6224569b0c43c15b0f75d4e03bbce38e856de623758c332d8972e9bbf9163b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Meta-Llama-3-70B-Instruct-Q3_K_S.gguf%3B+filename%3D%22Meta-Llama-3-70B-Instruct-Q3_K_S.gguf%22%3B&Expires=1713799050&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzc5OTA1MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzJhL2IwLzJhYjA0Y2IzMjk0MzI2ZDgyNTQ0ZThiOGNjZGQ1MWJkY2YwYjNlMjQzZTNmNzE1YTUyOGYyZmJhYWUwZDhmNDcvOGU2MjI0NTY5YjBjNDNjMTViMGY3NWQ0ZTAzYmJjZTM4ZTg1NmRlNjIzNzU4YzMzMmQ4OTcyZTliYmY5MTYzYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=PhE6oes-v6cAww2DjUxO-mXL5L%7EndBFEJkgrjWx4uAdi06ANXID27CWVV5eDKcycBERabbNzChQB4UtKCZkWbW0BcDV5ruyBjCFLLvChJ-SpZ2EcFORY5dRoBUTVwsW0HE5ZNMPGqNl3LHM2TSkyBnrGg8lKxjNLzyGWKlp0JNZPT53xdw2KlleqreeYLX4ISLvgnum0GTmsvvY%7EvYnX9uIX83fD%7E80cqSI0ekjyd5p8CpCmp6iBYBy%7E-RdeFN0B-kuTfPqQjFKbgshGLMZD7r2Ge5KuaynAA34iErk%7EcOAUP1tf5AsWNjVvXQEZdlFWPPKzXx01UHM8d--d1v0hpA__&Key-Pair-Id=KCD77M1F0VK2B\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 108.157.194.6, 108.157.194.47, 108.157.194.71, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|108.157.194.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30912563648 (29G) [binary/octet-stream]\n",
      "Saving to: ‘Meta-Llama-3-70B-Instruct-Q3_K_S.gguf’\n",
      "\n",
      "Meta-Llama-3-70B-In 100%[===================>]  28.79G  10.4MB/s    in 44m 50s \n",
      "\n",
      "2024-04-19 18:02:20 (11.0 MB/s) - ‘Meta-Llama-3-70B-Instruct-Q3_K_S.gguf’ saved [30912563648/30912563648]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/NousResearch/Meta-Llama-3-70B-Instruct-GGUF/resolve/main/Meta-Llama-3-70B-Instruct-Q3_K_S.gguf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the smaller one here\n",
    "\n",
    "Our draft model will be a Q3_K_L version of Llama-3-8B-Instruct. For even better result I suggest to pick something even faster. \n",
    "The hit rate will be crucial for the gain in performance. The greater the speed difference between the two models, the greater the potential gain. But the draft model must also be good enough to have a good hit rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-19 16:43:46--  https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf\n",
      "Resolving huggingface.co (huggingface.co)... 108.138.189.57, 108.138.189.70, 108.138.189.96, ...\n",
      "Connecting to huggingface.co (huggingface.co)|108.138.189.57|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/79/f2/79f21025e377180e4ec0e3968bca4612bb9c99fa84e70cb7815186c42a858124/1411591a3b405ef45313e92560e7a28920114a2a11a6e7ad79a36d9b58cc0084?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Meta-Llama-3-8B-Instruct.Q3_K_L.gguf%3B+filename%3D%22Meta-Llama-3-8B-Instruct.Q3_K_L.gguf%22%3B&Expires=1713797026&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzc5NzAyNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzc5L2YyLzc5ZjIxMDI1ZTM3NzE4MGU0ZWMwZTM5NjhiY2E0NjEyYmI5Yzk5ZmE4NGU3MGNiNzgxNTE4NmM0MmE4NTgxMjQvMTQxMTU5MWEzYjQwNWVmNDUzMTNlOTI1NjBlN2EyODkyMDExNGEyYTExYTZlN2FkNzlhMzZkOWI1OGNjMDA4ND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=ZNNE%7EG1sLvkDOCXwz00Rkr1cpn9eJ2GRtxx1i6X58T3G3WLc3J7HX4LBai-zEK6PMgPMPfM0v-0iRu6bX1kbyADXi0GGeQgXdqD-QF6D-F4fNeeiaMHoEd4N17ZUUWGFhBnSERPEL3HCYU4RN%7EmN7ZzJuGI5hmcuvXYK2Nqg4KJvahbNxK0bq2Jtw0YwapSk3rzQCbadrJfWz16LMyImm%7EnUmfVN3zbP2GvxRIK4WCKOi79uGhA34Qftfw4IDiqOC7KaKayRAaqImDkS5OOpufVG8XTvPabSpN-3e-s640SCTsKygmTgc4KLeekNVqXUhHBQTDigAhYfmsB39Ir6wg__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
      "--2024-04-19 16:43:46--  https://cdn-lfs-us-1.huggingface.co/repos/79/f2/79f21025e377180e4ec0e3968bca4612bb9c99fa84e70cb7815186c42a858124/1411591a3b405ef45313e92560e7a28920114a2a11a6e7ad79a36d9b58cc0084?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Meta-Llama-3-8B-Instruct.Q3_K_L.gguf%3B+filename%3D%22Meta-Llama-3-8B-Instruct.Q3_K_L.gguf%22%3B&Expires=1713797026&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzc5NzAyNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzc5L2YyLzc5ZjIxMDI1ZTM3NzE4MGU0ZWMwZTM5NjhiY2E0NjEyYmI5Yzk5ZmE4NGU3MGNiNzgxNTE4NmM0MmE4NTgxMjQvMTQxMTU5MWEzYjQwNWVmNDUzMTNlOTI1NjBlN2EyODkyMDExNGEyYTExYTZlN2FkNzlhMzZkOWI1OGNjMDA4ND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=ZNNE%7EG1sLvkDOCXwz00Rkr1cpn9eJ2GRtxx1i6X58T3G3WLc3J7HX4LBai-zEK6PMgPMPfM0v-0iRu6bX1kbyADXi0GGeQgXdqD-QF6D-F4fNeeiaMHoEd4N17ZUUWGFhBnSERPEL3HCYU4RN%7EmN7ZzJuGI5hmcuvXYK2Nqg4KJvahbNxK0bq2Jtw0YwapSk3rzQCbadrJfWz16LMyImm%7EnUmfVN3zbP2GvxRIK4WCKOi79uGhA34Qftfw4IDiqOC7KaKayRAaqImDkS5OOpufVG8XTvPabSpN-3e-s640SCTsKygmTgc4KLeekNVqXUhHBQTDigAhYfmsB39Ir6wg__&Key-Pair-Id=KCD77M1F0VK2B\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 108.157.194.71, 108.157.194.47, 108.157.194.70, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|108.157.194.71|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4322469088 (4.0G) [binary/octet-stream]\n",
      "Saving to: ‘Meta-Llama-3-8B-Instruct.Q3_K_L.gguf.1’\n",
      "\n",
      "lama-3-8B-Instruct.   0%[                    ]  35.96M  11.3MB/s    eta 6m 8s  ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test how fast is the larger model alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a polite chatbot who always responds in Italian!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nMake me a summary of the Napoleon life<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a polite chatbot who always responds in Italian!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Make me a summary of the Napoleon life\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "offload to GPU 45 level. \n",
    "It is important to remember that Llama-3-70B has about 80 levels. Loading all 80 levels is not possible on a single 4090 despite the Q3_K_L quantized version. We will also need some space to load the draft model into the GPU as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 2697 (9958c81b)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1713772474\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /home/nlp/grimaldian/llms-lab/pratical-llms/Meta-Llama-3-70B-Instruct-Q3_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = ..\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 28672\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 11\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q3_K:  481 tensors\n",
      "llama_model_loader: - type q5_K:   80 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 70B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Small\n",
      "llm_load_print_meta: model params     = 70.55 B\n",
      "llm_load_print_meta: model size       = 28.78 GiB (3.50 BPW) \n",
      "llm_load_print_meta: general.name     = ..\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.74 MiB\n",
      "llm_load_tensors: offloading 45 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 45/81 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 29472.53 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 15873.75 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =    70.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    90.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =  1088.45 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    17.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2566\n",
      "llama_new_context_with_model: graph splits = 389\n",
      "\n",
      "system_info: n_threads = 4 / 4 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.000\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 2048, n_predict = 100, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33msystem\n",
      "\n",
      "You are a polite chatbot who always responds in Italian!user\n",
      "\n",
      "Make me a summary of the Napoleon lifeassistant\n",
      "\n",
      "\u001b[0mBuongiorno! Sono felice di fornirle una sintesi della vita di Napoleone Bonaparte.\n",
      "\n",
      "Napoleone nacque il 15 agosto 1769 ad Ajaccio, Corsica. Figlio di Carlo Buonaparte e Letizia Ramolino, crebbe in una famiglia di nobili corsi. Nel 1785 entrò nella scuola militare di Parigi, dove si diplomò nel 1786.\n",
      "\n",
      "\n",
      "llama_print_timings:        load time =   22809.92 ms\n",
      "llama_print_timings:      sample time =      11.13 ms /   100 runs   (    0.11 ms per token,  8986.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1921.27 ms /    37 tokens (   51.93 ms per token,    19.26 tokens per second)\n",
      "llama_print_timings:        eval time =   47780.49 ms /    99 runs   (  482.63 ms per token,     2.07 tokens per second)\n",
      "llama_print_timings:       total time =   49936.35 ms /   136 tokens\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model_path = os.getcwd() + \"/Meta-Llama-3-8B-Instruct.Q8_0.gguf\"\n",
    "model_path = os.getcwd() + \"/Meta-Llama-3-70B-Instruct-Q3_K_S.gguf\"\n",
    "!./llama.cpp/main -m {model_path} -n 100 --color --temp 0.0 -ngl 50 -p \"{prompt}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "llama_print_timings:        load time =   22809.92 ms\n",
    "llama_print_timings:      sample time =      11.13 ms /   100 runs   (    0.11 ms per token,  8986.34 tokens per second)\n",
    "llama_print_timings: prompt eval time =    1921.27 ms /    37 tokens (   51.93 ms per token,    19.26 tokens per second)\n",
    "llama_print_timings:        eval time =   47780.49 ms /    99 runs   (  482.63 ms per token,     2.07 tokens per second)\n",
    "llama_print_timings:       total time =   49936.35 ms /   136 tokens\n",
    "```\n",
    "\n",
    "Total time of 136 token in 49,9 seconds. \n",
    "### 2.72 token/second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try the smaller model only first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 2697 (9958c81b)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1713772666\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/nlp/grimaldian/llms-lab/pratical-llms/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 13\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q3_K:  129 tensors\n",
      "llama_model_loader: - type q5_K:   96 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Large\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.02 GiB (4.30 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   215.27 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  3898.99 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     9.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "\n",
      "system_info: n_threads = 4 / 4 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.000\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 2048, n_predict = 100, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33msystem\n",
      "\n",
      "You are a polite chatbot who always responds in Italian!user\n",
      "\n",
      "Make me a summary of the Napoleon lifeassistant\n",
      "\n",
      "\u001b[0mCiao! Sono felice di fornire una breve sintesi della vita di Napoleone Bonaparte.\n",
      "\n",
      "Napoleone Bonaparte nacque il 15 agosto 1769 a Ajaccio, Corsica. Era il quinto figlio di Carlo Buonaparte e di Letizia Ramolino. Dopo aver frequentato l'École Militaire di Parigi, Napoleone si unì all'esercito francese e partecip\n",
      "llama_print_timings:        load time =    3522.16 ms\n",
      "llama_print_timings:      sample time =      10.60 ms /   100 runs   (    0.11 ms per token,  9435.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =      45.31 ms /    37 tokens (    1.22 ms per token,   816.60 tokens per second)\n",
      "llama_print_timings:        eval time =    1162.92 ms /    99 runs   (   11.75 ms per token,    85.13 tokens per second)\n",
      "llama_print_timings:       total time =    1285.67 ms /   136 tokens\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "model_path = os.getcwd() + \"/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf\"\n",
    "!./llama.cpp/main -m {model_path} -n 100 --color --temp 0.0 -ngl 99 -ngld 100 -p \"{prompt}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "llama_print_timings:        load time =    3522.16 ms\n",
    "llama_print_timings:      sample time =      10.60 ms /   100 runs   (    0.11 ms per token,  9435.74 tokens per second)\n",
    "llama_print_timings: prompt eval time =      45.31 ms /    37 tokens (    1.22 ms per token,   816.60 tokens per second)\n",
    "llama_print_timings:        eval time =    1162.92 ms /    99 runs   (   11.75 ms per token,    85.13 tokens per second)\n",
    "llama_print_timings:       total time =    1285.67 ms /   136 tokens\n",
    "```\n",
    "\n",
    "The smallest version is pretty fast. Total time of 136 tokes in 1,28 seconds. \n",
    "### 105,83 token/second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try Speculative decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_model_path = os.getcwd() + \"/Meta-Llama-3-70B-Instruct-Q3_K_S.gguf\"\n",
    "smaller_model_path = os.getcwd() + \"/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /home/nlp/grimaldian/llms-lab/pratical-llms/Meta-Llama-3-70B-Instruct-Q3_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = ..\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 28672\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 11\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q3_K:  481 tensors\n",
      "llama_model_loader: - type q5_K:   80 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 70B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Small\n",
      "llm_load_print_meta: model params     = 70.55 B\n",
      "llm_load_print_meta: model size       = 28.78 GiB (3.50 BPW) \n",
      "llm_load_print_meta: general.name     = ..\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.74 MiB\n",
      "llm_load_tensors: offloading 45 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 45/81 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 29472.53 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 15873.75 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =    70.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    90.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =  1088.45 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    17.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2566\n",
      "llama_new_context_with_model: graph splits = 389\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/nlp/grimaldian/llms-lab/pratical-llms/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 13\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q3_K:  129 tensors\n",
      "llama_model_loader: - type q5_K:   96 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Large\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.02 GiB (4.30 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   215.27 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  3898.99 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     9.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "\n",
      "\n",
      "system\n",
      "\n",
      "You are a polite chatbot who always responds in Italian!user\n",
      "\n",
      "Make me a summary of the Napoleon lifeassistant\n",
      "\n",
      "Buong\u001b[36mi\u001b[37m\u001b[36morno\u001b[37m\u001b[36m!\u001b[37m S\u001b[36mono\u001b[37m\u001b[36m fel\u001b[37m\u001b[36mice\u001b[37m di\u001b[36m forn\u001b[37mir\u001b[36mle\u001b[37m\u001b[36m una\u001b[37m sint\u001b[36mesi\u001b[37m\u001b[36m della\u001b[37m\u001b[36m vita\u001b[37m di\u001b[36m Nap\u001b[37m\u001b[36mole\u001b[37m\u001b[36mone\u001b[37m Bon\u001b[36map\u001b[37m\u001b[36marte\u001b[37m\u001b[36m.\n",
      "\n",
      "\u001b[37mN\u001b[36map\u001b[37m\u001b[36mole\u001b[37m\u001b[36mone\u001b[37m n\u001b[36mac\u001b[37m\u001b[36mque\u001b[37m il\u001b[36m \u001b[37m\u001b[36m15\u001b[37m\u001b[36m agosto\u001b[37m \u001b[36m176\u001b[37m\u001b[36m9\u001b[37m ad\u001b[36m Aj\u001b[37m\u001b[36macc\u001b[37m\u001b[36mio\u001b[37m,\u001b[36m Cors\u001b[37m\u001b[36mica\u001b[37m. Fig\u001b[36mlio\u001b[37m\u001b[36m di\u001b[37m\u001b[36m Carlo\u001b[37m Bu\u001b[36mon\u001b[37m\u001b[36map\u001b[37m\u001b[36marte\u001b[37m e Let\u001b[36mizia\u001b[37m\u001b[36m Ram\u001b[37m\u001b[36mol\u001b[37mino\u001b[36m,\u001b[37m cre\u001b[36mbbe\u001b[37m\u001b[36m in\u001b[37m una\u001b[36m fam\u001b[37m\u001b[36miglia\u001b[37m\u001b[36m di\u001b[37m nob\u001b[36mili\u001b[37m cors\u001b[36mi\u001b[37m\u001b[36m.\u001b[37m Nel\u001b[36m \u001b[37m\u001b[36m178\u001b[37m\u001b[36m5\u001b[37m entr\u001b[36mò\u001b[37m nella sc\u001b[36mu\u001b[37m\u001b[36mola\u001b[37m\u001b[36m milit\u001b[37mare\u001b[36m di\u001b[37m Par\u001b[36migi\u001b[37m\u001b[36m,\u001b[37m\u001b[36m dove\u001b[37m si\u001b[36m diplom\u001b[37m\u001b[36mò\u001b[37m\u001b[36m nel\u001b[37m \u001b[36m178\u001b[37m6.\n",
      "\n",
      "In\n",
      "\n",
      "encoded   37 tokens in    4.136 seconds, speed:    8.946 t/s\n",
      "decoded  101 tokens in   41.043 seconds, speed:    2.461 t/s\n",
      "\n",
      "n_draft   = 3\n",
      "n_predict = 101\n",
      "n_drafted = 102\n",
      "n_accept  = 66\n",
      "accept    = 64.706%\n",
      "\n",
      "draft:\n",
      "\n",
      "llama_print_timings:        load time =    3369.11 ms\n",
      "llama_print_timings:      sample time =     579.74 ms /     1 runs   (  579.74 ms per token,     1.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38172.09 ms /   104 tokens (  367.04 ms per token,     2.72 tokens per second)\n",
      "llama_print_timings:        eval time =    1086.22 ms /    68 runs   (   15.97 ms per token,    62.60 tokens per second)\n",
      "llama_print_timings:       total time =   45198.67 ms /   172 tokens\n",
      "\n",
      "target:\n",
      "\n",
      "llama_print_timings:        load time =   78407.54 ms\n",
      "llama_print_timings:      sample time =      10.45 ms /   101 runs   (    0.10 ms per token,  9668.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =   42626.22 ms /   173 tokens (  246.39 ms per token,     4.06 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   48942.17 ms /   174 tokens\n",
      "\n",
      "\n",
      "CPU times: user 1.26 s, sys: 273 ms, total: 1.54 s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!./llama.cpp/speculative -ngl 45 -ngld 100 --color -m  {larger_model_path} --model-draft {smaller_model_path}  -p \"{prompt}\" --temp 0.0 -n 100 -s 1 --top-k 0 --top-p 1 --repeat-last-n 0 --repeat-penalty 1.0 --draft 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "draft:\n",
    "\n",
    "llama_print_timings:        load time =    3226.00 ms\n",
    "llama_print_timings:      sample time =     577.99 ms /     1 runs   (  577.99 ms per token,     1.73 tokens per second)\n",
    "llama_print_timings: prompt eval time =   37783.48 ms /   104 tokens (  363.30 ms per token,     2.75 tokens per second)\n",
    "llama_print_timings:        eval time =    1043.41 ms /    68 runs   (   15.34 ms per token,    65.17 tokens per second)\n",
    "llama_print_timings:       total time =   43304.43 ms /   172 tokens\n",
    "\n",
    "target:\n",
    "\n",
    "llama_print_timings:        load time =   23264.80 ms\n",
    "llama_print_timings:      sample time =      10.31 ms /   101 runs   (    0.10 ms per token,  9793.46 tokens per second)\n",
    "llama_print_timings: prompt eval time =   40794.61 ms /   173 tokens (  235.81 ms per token,     4.24 tokens per second)\n",
    "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
    "llama_print_timings:       total time =   46833.24 ms /   174 tokens\n",
    "```\n",
    "\n",
    "174 tokens on 46,8 second total. \n",
    "### Total time of 3.71 tokens/seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up the results\n",
    "\n",
    "The speculative sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                              | GPU Layers offload N | Total Tokens/seconds                                                                                                                                                                       | Gain %  |\n",
    "|--------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------|-----|\n",
    "| Meta-Llama-3-70B-Instruct-Q3_K_S.gguf | 45/80 | 2.72 | - |\n",
    "| Meta-Llama-3-8B-Instruct.Q3_K_L.gguf  | 32/32  | 105.83 | - |\n",
    "| Speculative Sampling   | 45/80 and 32/32 | 3.71 | 36% |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
