{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speculative Decoding\n",
    "While small models are very fast and allow for very high tokens/second even on commercial hardware, their performance is not always high especially for specific use cases. On the contrary, larger models can offer greater performance guarantees but not good latency and acceptable response times. A technique to increase speed and reduce latency of larger models is Speculative Decoding.\n",
    "\n",
    "The underlying idea of this technique is that many words are very easy to generate. Imagine, for example, all the articles or interconnection words present between facts. By being able to \"speculate\" and therefore guess the next N words, we can potentially reduce inference times.\n",
    "\n",
    "This happens because usually we are forced to perform an entire forward pass on the entire input sequence to generate the next token, which is then inserted into the input sequence. The process is repeated N times to generate the next N tokens. In this scenario, every time we need to predict a token, we have to propagate the input sequence through all the layers of the transformers. It is not necessary to recalculate everything completely thanks to some optimization techniques like the KV cache, which allows avoiding the recalculation of attention for all tokens up to the last one inserted. But regardless of this aspect, it is important to understand that it takes almost the same time to generate a single token or to propagate an entire new sequence of tokens to \"verify\" it (this is not exactly true for the discussion I mentioned earlier about the KV cache, but let's assume for simplicity that it is).\n",
    "\n",
    "![spec_1.png](images/spec_1.png)\n",
    "\n",
    "The algorithm is as follows:\n",
    "- Generate the next K tokens with the Draft Model (the fast model).\n",
    "- Run the target model in parallel on the input sequence + the new generated tokens (speculative tokens). All probability distributions of the new tokens are calculated in parallel. To be convinced of this, just review how a forward pass works on a transformer architecture.\n",
    "- The next step is Rejection Sampling, in which we decide whether to accept or reject each individual token produced by the Draft Model. Each token is checked sequentially and added to the queue of accepted tokens. As soon as a token to be rejected is encountered, all subsequent tokens are discarded and the loop exits. Since I have already calculated all probability distributions up to that point, I can generate and sample the correct token and continue with the process.\n",
    "\n",
    "The token is accepted if the target model is as confident as or more confident than the Draft Model in sampling that token. If the sampling probability is lower, that token is accepted with a certain probability that may depend on the type of Speculative Sampling algorithm we are using.\n",
    "\n",
    "![spec_2.png](images/spec_2.png)\n",
    "\n",
    "I won't go into too much detail about the algorithm, but for a clear understanding of how it works, I suggest watching the following YouTube video by Efficient NLP (https://www.youtube.com/watch?v=S-8yr_RibJ4)\n",
    "\n",
    "\n",
    "\n",
    "The best case scenario occurs when all tokens are accepted; we will have generated N tokens at the speed of the draft model plus a single forward pass of the target model.\n",
    "\n",
    "The worst case scenario occurs when all tokens are rejected. In that case, the time it takes for the target model to generate a single token will be worsened by the time it takes for the draft model to generate N tokens (plus the verification time given by the algorithm). This helps us understand how important it is to have a good hit rate (given by the percentage of accepted tokens and not rejected) and also tells us that it's crucial to have a good Draft Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this if you haven't already done it\n",
    "```\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
    "!pip install -r llama.cpp/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Can we use a smaller (and faster) quantized model as speculative model for the bigger one?\n",
    "Let's see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the bigger model here\n",
    "Let's test using the newest LLama3-70B-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-19 17:17:30--  https://huggingface.co/NousResearch/Meta-Llama-3-70B-Instruct-GGUF/resolve/main/Meta-Llama-3-70B-Instruct-Q3_K_S.gguf\n",
      "Resolving huggingface.co (huggingface.co)... 108.138.189.74, 108.138.189.57, 108.138.189.70, ...\n",
      "Connecting to huggingface.co (huggingface.co)|108.138.189.74|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/2a/b0/2ab04cb3294326d82544e8b8ccdd51bdcf0b3e243e3f715a528f2fbaae0d8f47/8e6224569b0c43c15b0f75d4e03bbce38e856de623758c332d8972e9bbf9163b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Meta-Llama-3-70B-Instruct-Q3_K_S.gguf%3B+filename%3D%22Meta-Llama-3-70B-Instruct-Q3_K_S.gguf%22%3B&Expires=1713799050&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzc5OTA1MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzJhL2IwLzJhYjA0Y2IzMjk0MzI2ZDgyNTQ0ZThiOGNjZGQ1MWJkY2YwYjNlMjQzZTNmNzE1YTUyOGYyZmJhYWUwZDhmNDcvOGU2MjI0NTY5YjBjNDNjMTViMGY3NWQ0ZTAzYmJjZTM4ZTg1NmRlNjIzNzU4YzMzMmQ4OTcyZTliYmY5MTYzYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=PhE6oes-v6cAww2DjUxO-mXL5L%7EndBFEJkgrjWx4uAdi06ANXID27CWVV5eDKcycBERabbNzChQB4UtKCZkWbW0BcDV5ruyBjCFLLvChJ-SpZ2EcFORY5dRoBUTVwsW0HE5ZNMPGqNl3LHM2TSkyBnrGg8lKxjNLzyGWKlp0JNZPT53xdw2KlleqreeYLX4ISLvgnum0GTmsvvY%7EvYnX9uIX83fD%7E80cqSI0ekjyd5p8CpCmp6iBYBy%7E-RdeFN0B-kuTfPqQjFKbgshGLMZD7r2Ge5KuaynAA34iErk%7EcOAUP1tf5AsWNjVvXQEZdlFWPPKzXx01UHM8d--d1v0hpA__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
      "--2024-04-19 17:17:31--  https://cdn-lfs-us-1.huggingface.co/repos/2a/b0/2ab04cb3294326d82544e8b8ccdd51bdcf0b3e243e3f715a528f2fbaae0d8f47/8e6224569b0c43c15b0f75d4e03bbce38e856de623758c332d8972e9bbf9163b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Meta-Llama-3-70B-Instruct-Q3_K_S.gguf%3B+filename%3D%22Meta-Llama-3-70B-Instruct-Q3_K_S.gguf%22%3B&Expires=1713799050&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzc5OTA1MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzJhL2IwLzJhYjA0Y2IzMjk0MzI2ZDgyNTQ0ZThiOGNjZGQ1MWJkY2YwYjNlMjQzZTNmNzE1YTUyOGYyZmJhYWUwZDhmNDcvOGU2MjI0NTY5YjBjNDNjMTViMGY3NWQ0ZTAzYmJjZTM4ZTg1NmRlNjIzNzU4YzMzMmQ4OTcyZTliYmY5MTYzYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=PhE6oes-v6cAww2DjUxO-mXL5L%7EndBFEJkgrjWx4uAdi06ANXID27CWVV5eDKcycBERabbNzChQB4UtKCZkWbW0BcDV5ruyBjCFLLvChJ-SpZ2EcFORY5dRoBUTVwsW0HE5ZNMPGqNl3LHM2TSkyBnrGg8lKxjNLzyGWKlp0JNZPT53xdw2KlleqreeYLX4ISLvgnum0GTmsvvY%7EvYnX9uIX83fD%7E80cqSI0ekjyd5p8CpCmp6iBYBy%7E-RdeFN0B-kuTfPqQjFKbgshGLMZD7r2Ge5KuaynAA34iErk%7EcOAUP1tf5AsWNjVvXQEZdlFWPPKzXx01UHM8d--d1v0hpA__&Key-Pair-Id=KCD77M1F0VK2B\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 108.157.194.6, 108.157.194.47, 108.157.194.71, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|108.157.194.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30912563648 (29G) [binary/octet-stream]\n",
      "Saving to: ‘Meta-Llama-3-70B-Instruct-Q3_K_S.gguf’\n",
      "\n",
      "Meta-Llama-3-70B-In 100%[===================>]  28.79G  10.4MB/s    in 44m 50s \n",
      "\n",
      "2024-04-19 18:02:20 (11.0 MB/s) - ‘Meta-Llama-3-70B-Instruct-Q3_K_S.gguf’ saved [30912563648/30912563648]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/NousResearch/Meta-Llama-3-70B-Instruct-GGUF/resolve/main/Meta-Llama-3-70B-Instruct-Q3_K_S.gguf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the smaller one here\n",
    "\n",
    "Our draft model will be a Q3_K_L version of Llama-3-8B-Instruct. For even better result I suggest to pick something even faster. \n",
    "The hit rate will be crucial for the gain in performance. The greater the speed difference between the two models, the greater the potential gain. But the draft model must also be good enough to have a good hit rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-19 16:43:46--  https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf\n",
      "Resolving huggingface.co (huggingface.co)... 108.138.189.57, 108.138.189.70, 108.138.189.96, ...\n",
      "Connecting to huggingface.co (huggingface.co)|108.138.189.57|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/79/f2/79f21025e377180e4ec0e3968bca4612bb9c99fa84e70cb7815186c42a858124/1411591a3b405ef45313e92560e7a28920114a2a11a6e7ad79a36d9b58cc0084?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Meta-Llama-3-8B-Instruct.Q3_K_L.gguf%3B+filename%3D%22Meta-Llama-3-8B-Instruct.Q3_K_L.gguf%22%3B&Expires=1713797026&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzc5NzAyNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzc5L2YyLzc5ZjIxMDI1ZTM3NzE4MGU0ZWMwZTM5NjhiY2E0NjEyYmI5Yzk5ZmE4NGU3MGNiNzgxNTE4NmM0MmE4NTgxMjQvMTQxMTU5MWEzYjQwNWVmNDUzMTNlOTI1NjBlN2EyODkyMDExNGEyYTExYTZlN2FkNzlhMzZkOWI1OGNjMDA4ND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=ZNNE%7EG1sLvkDOCXwz00Rkr1cpn9eJ2GRtxx1i6X58T3G3WLc3J7HX4LBai-zEK6PMgPMPfM0v-0iRu6bX1kbyADXi0GGeQgXdqD-QF6D-F4fNeeiaMHoEd4N17ZUUWGFhBnSERPEL3HCYU4RN%7EmN7ZzJuGI5hmcuvXYK2Nqg4KJvahbNxK0bq2Jtw0YwapSk3rzQCbadrJfWz16LMyImm%7EnUmfVN3zbP2GvxRIK4WCKOi79uGhA34Qftfw4IDiqOC7KaKayRAaqImDkS5OOpufVG8XTvPabSpN-3e-s640SCTsKygmTgc4KLeekNVqXUhHBQTDigAhYfmsB39Ir6wg__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
      "--2024-04-19 16:43:46--  https://cdn-lfs-us-1.huggingface.co/repos/79/f2/79f21025e377180e4ec0e3968bca4612bb9c99fa84e70cb7815186c42a858124/1411591a3b405ef45313e92560e7a28920114a2a11a6e7ad79a36d9b58cc0084?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Meta-Llama-3-8B-Instruct.Q3_K_L.gguf%3B+filename%3D%22Meta-Llama-3-8B-Instruct.Q3_K_L.gguf%22%3B&Expires=1713797026&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzc5NzAyNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzc5L2YyLzc5ZjIxMDI1ZTM3NzE4MGU0ZWMwZTM5NjhiY2E0NjEyYmI5Yzk5ZmE4NGU3MGNiNzgxNTE4NmM0MmE4NTgxMjQvMTQxMTU5MWEzYjQwNWVmNDUzMTNlOTI1NjBlN2EyODkyMDExNGEyYTExYTZlN2FkNzlhMzZkOWI1OGNjMDA4ND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=ZNNE%7EG1sLvkDOCXwz00Rkr1cpn9eJ2GRtxx1i6X58T3G3WLc3J7HX4LBai-zEK6PMgPMPfM0v-0iRu6bX1kbyADXi0GGeQgXdqD-QF6D-F4fNeeiaMHoEd4N17ZUUWGFhBnSERPEL3HCYU4RN%7EmN7ZzJuGI5hmcuvXYK2Nqg4KJvahbNxK0bq2Jtw0YwapSk3rzQCbadrJfWz16LMyImm%7EnUmfVN3zbP2GvxRIK4WCKOi79uGhA34Qftfw4IDiqOC7KaKayRAaqImDkS5OOpufVG8XTvPabSpN-3e-s640SCTsKygmTgc4KLeekNVqXUhHBQTDigAhYfmsB39Ir6wg__&Key-Pair-Id=KCD77M1F0VK2B\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 108.157.194.71, 108.157.194.47, 108.157.194.70, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|108.157.194.71|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4322469088 (4.0G) [binary/octet-stream]\n",
      "Saving to: ‘Meta-Llama-3-8B-Instruct.Q3_K_L.gguf.1’\n",
      "\n",
      "lama-3-8B-Instruct.   0%[                    ]  35.96M  11.3MB/s    eta 6m 8s  ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test how fast is the larger model alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/grimaldian/llms-lab/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a polite chatbot who always responds the user requests<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nMake me a summary of the Napoleon life<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a polite chatbot who always responds the user requests\"},\n",
    "    {\"role\": \"user\", \"content\": \"Make me a summary of the Napoleon life\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "offload to GPU 45 level. \n",
    "It is important to remember that Llama-3-70B has about 80 levels. Loading all 80 levels is not possible on a single 4090 despite the Q3_K_L quantized version. We will also need some space to load the draft model into the GPU as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 2697 (9958c81b)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1713783078\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /home/nlp/grimaldian/llms-lab/pratical-llms/Meta-Llama-3-70B-Instruct-Q3_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = ..\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 28672\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 11\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q3_K:  481 tensors\n",
      "llama_model_loader: - type q5_K:   80 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 70B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Small\n",
      "llm_load_print_meta: model params     = 70.55 B\n",
      "llm_load_print_meta: model size       = 28.78 GiB (3.50 BPW) \n",
      "llm_load_print_meta: general.name     = ..\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.74 MiB\n",
      "llm_load_tensors: offloading 45 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 45/81 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 29472.53 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 15873.75 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =    70.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    90.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =  1088.45 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    17.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2566\n",
      "llama_new_context_with_model: graph splits = 389\n",
      "\n",
      "system_info: n_threads = 4 / 4 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 1, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.000\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 2048, n_predict = 250, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33msystem\n",
      "\n",
      "You are a polite chatbot who always responds the user requestsuser\n",
      "\n",
      "Make me a summary of the Napoleon lifeassistant\n",
      "\n",
      "\u001b[0mBonjour! I'd be delighted to provide you with a summary of Napoleon Bonaparte's life.\n",
      "\n",
      "**Early Life (1769-1796)**\n",
      "\n",
      "Napoleon Bonaparte was born on August 15, 1769, in Ajaccio, Corsica, to Carlo Buonaparte and Letizia Ramolino. He was the fourth of eleven children. Napoleon's family was of minor Corsican nobility. He attended the École Militaire in Paris, where he excelled in mathematics and graduated in 1785.\n",
      "\n",
      "**Rise to Power (1796-1804)**\n",
      "\n",
      "Napoleon became a captain in the French army and gained recognition for his bravery during the Siege of Toulon in 1793. He married Joséphine de Beauharnais in 1796. Napoleon's military successes, including the Italian Campaign (1796-1797) and the Egyptian Campaign (1798-1801), made him a national hero. He returned to France and overthrew the Directory, a group of five leaders, in the Coup d'État of 18 Brumaire (November 9, 1799). Napoleon became the First Consul of France, effectively\n",
      "llama_print_timings:        load time =   30138.03 ms\n",
      "llama_print_timings:      sample time =      26.52 ms /   250 runs   (    0.11 ms per token,  9427.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3023.35 ms /    37 tokens (   81.71 ms per token,    12.24 tokens per second)\n",
      "llama_print_timings:        eval time =  123909.01 ms /   249 runs   (  497.63 ms per token,     2.01 tokens per second)\n",
      "llama_print_timings:       total time =  127313.45 ms /   286 tokens\n",
      "Log end\n",
      "CPU times: user 1.58 s, sys: 184 ms, total: 1.77 s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model_path = os.getcwd() + \"/Meta-Llama-3-8B-Instruct.Q8_0.gguf\"\n",
    "model_path = os.getcwd() + \"/Meta-Llama-3-70B-Instruct-Q3_K_S.gguf\"\n",
    "!./llama.cpp/main -m {model_path} -n 250 --color --temp 0.0 -ngl 45 --top-k 1 -e --temp -1 -p \"{prompt}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.01 token/second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try the smaller model only first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 2697 (9958c81b)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1713783238\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/nlp/grimaldian/llms-lab/pratical-llms/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 13\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q3_K:  129 tensors\n",
      "llama_model_loader: - type q5_K:   96 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Large\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.02 GiB (4.30 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   215.27 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  3898.99 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     9.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "\n",
      "system_info: n_threads = 4 / 4 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 1, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.000\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 2048, n_predict = 250, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33msystem\n",
      "\n",
      "You are a polite chatbot who always responds the user requestsuser\n",
      "\n",
      "Make me a summary of the Napoleon lifeassistant\n",
      "\n",
      "\u001b[0mA fascinating topic! Here's a summary of the life of Napoleon Bonaparte:\n",
      "\n",
      "**Early Life (1769-1795)**\n",
      "\n",
      "Napoleon was born on August 15, 1769, in Ajaccio, Corsica. He was the fourth child of Carlo Buonaparte and Letizia Ramolino. Napoleon studied at the École Militaire in Paris and graduated in 1785. He became a lieutenant in the French army and served in Italy and Egypt.\n",
      "\n",
      "**Rise to Power (1795-1804)**\n",
      "\n",
      "Napoleon's military skills and strategic thinking quickly gained attention. He became a key figure in the French Revolution, rising through the ranks to become a general. In 1799, he led a coup d'état, overthrowing the Directory and establishing the Consulate, with himself as the de facto ruler.\n",
      "\n",
      "**Consul and Emperor (1804-1815)**\n",
      "\n",
      "In 1804, Napoleon was crowned Emperor of the French, marking the beginning of the Napoleonic Empire. He reorganized Europe, creating a vast empire that included much of Western and Central Europe. He introduced the Napoleonic Code, a comprehensive set of laws that reformed the French legal system\n",
      "llama_print_timings:        load time =    3833.27 ms\n",
      "llama_print_timings:      sample time =      30.18 ms /   250 runs   (    0.12 ms per token,  8283.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =      46.77 ms /    37 tokens (    1.26 ms per token,   791.11 tokens per second)\n",
      "llama_print_timings:        eval time =    2861.28 ms /   249 runs   (   11.49 ms per token,    87.02 tokens per second)\n",
      "llama_print_timings:       total time =    3109.85 ms /   286 tokens\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "model_path = os.getcwd() + \"/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf\"\n",
    "!./llama.cpp/main -m {model_path} -n 250 --color --temp 0.0 -ngl 99 --top-k 1 -p \"{prompt}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smallest version is pretty fast. \n",
    "### 87.02 token/second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try Speculative decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_model_path = os.getcwd() + \"/Meta-Llama-3-70B-Instruct-Q3_K_S.gguf\"\n",
    "smaller_model_path = os.getcwd() + \"/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /home/nlp/grimaldian/llms-lab/pratical-llms/Meta-Llama-3-70B-Instruct-Q3_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = ..\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 28672\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 11\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q3_K:  481 tensors\n",
      "llama_model_loader: - type q5_K:   80 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 70B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Small\n",
      "llm_load_print_meta: model params     = 70.55 B\n",
      "llm_load_print_meta: model size       = 28.78 GiB (3.50 BPW) \n",
      "llm_load_print_meta: general.name     = ..\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.74 MiB\n",
      "llm_load_tensors: offloading 45 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 45/81 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 29472.53 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 15873.75 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =    70.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    90.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =  1088.45 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    17.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2566\n",
      "llama_new_context_with_model: graph splits = 389\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/nlp/grimaldian/llms-lab/pratical-llms/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 13\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q3_K:  129 tensors\n",
      "llama_model_loader: - type q5_K:   96 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Large\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.02 GiB (4.30 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   215.27 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  3898.99 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     9.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "\n",
      "\n",
      "system\n",
      "\n",
      "You are a polite chatbot who always responds the user requestsuser\n",
      "\n",
      "Make me a summary of the Napoleon lifeassistant\n",
      "\n",
      "Bonjour\u001b[36m!\u001b[37m\u001b[36m I\u001b[37m\u001b[36m'd\u001b[37m be\u001b[36m delighted\u001b[37m\u001b[36m to\u001b[37m\u001b[36m provide\u001b[37m you\u001b[36m with\u001b[37m\u001b[36m a\u001b[37m\u001b[36m summary\u001b[37m of Napoleon\u001b[36m Bon\u001b[37m\u001b[36map\u001b[37m\u001b[36marte\u001b[37m's\u001b[36m life\u001b[37m\u001b[36m.\n",
      "\n",
      "\u001b[37m**\u001b[36mEarly\u001b[37m\u001b[36m Life\u001b[37m (\u001b[36m176\u001b[37m\u001b[36m9\u001b[37m\u001b[36m-\u001b[37m1796\u001b[36m)**\u001b[37m\u001b[36m\n",
      "\n",
      "\u001b[37m\u001b[36mN\u001b[37map\u001b[36moleon\u001b[37m\u001b[36m Bon\u001b[37m\u001b[36map\u001b[37marte\u001b[36m was\u001b[37m\u001b[36m born\u001b[37m\u001b[36m on\u001b[37m August\u001b[36m \u001b[37m\u001b[36m15\u001b[37m\u001b[36m,\u001b[37m \u001b[36m176\u001b[37m\u001b[36m9\u001b[37m\u001b[36m,\u001b[37m in\u001b[36m Aj\u001b[37m\u001b[36macc\u001b[37m\u001b[36mio\u001b[37m,\u001b[36m Cors\u001b[37m\u001b[36mica\u001b[37m,\u001b[36m to\u001b[37m\u001b[36m Carlo\u001b[37m\u001b[36m Bu\u001b[37mon\u001b[36map\u001b[37m\u001b[36marte\u001b[37m\u001b[36m and\u001b[37m Let\u001b[36mizia\u001b[37m\u001b[36m Ram\u001b[37m\u001b[36mol\u001b[37mino\u001b[36m.\u001b[37m\u001b[36m He\u001b[37m was\u001b[36m the\u001b[37m\u001b[36m fourth\u001b[37m\u001b[36m of\u001b[37m eleven\u001b[36m children\u001b[37m\u001b[36m.\u001b[37m\u001b[36m Napoleon\u001b[37m's\u001b[36m family\u001b[37m\u001b[36m was\u001b[37m\u001b[36m of\u001b[37m minor Cors\u001b[36mican\u001b[37m\u001b[36m nob\u001b[37m\u001b[36mility\u001b[37m.\u001b[36m He\u001b[37m attended\u001b[36m the\u001b[37m\u001b[36m É\u001b[37m\u001b[36mcole\u001b[37m Milit\u001b[36maire\u001b[37m\u001b[36m in\u001b[37m\u001b[36m Paris\u001b[37m,\u001b[36m where\u001b[37m\u001b[36m he\u001b[37m exc\u001b[36melled\u001b[37m in\u001b[36m mathematics\u001b[37m\u001b[36m and\u001b[37m\u001b[36m graduated\u001b[37m in\u001b[36m \u001b[37m\u001b[36m178\u001b[37m\u001b[36m5\u001b[37m.\n",
      "\n",
      "\u001b[36m**\u001b[37m\u001b[36mR\u001b[37m\u001b[36mise\u001b[37m to\u001b[36m Power\u001b[37m\u001b[36m (\u001b[37m\u001b[36m179\u001b[37m6\u001b[36m-\u001b[37m\u001b[36m180\u001b[37m\u001b[36m4\u001b[37m)**\u001b[36m\n",
      "\n",
      "\u001b[37m\u001b[36mN\u001b[37m\u001b[36map\u001b[37moleon became\u001b[36m a\u001b[37m captain\u001b[36m in\u001b[37m\u001b[36m the\u001b[37m\u001b[36m French\u001b[37m army and gained\u001b[36m recognition\u001b[37m\u001b[36m for\u001b[37m\u001b[36m his\u001b[37m bravery during\u001b[36m the\u001b[37m Siege\u001b[36m of\u001b[37m\u001b[36m T\u001b[37m\u001b[36moul\u001b[37mon\u001b[36m in\u001b[37m\u001b[36m \u001b[37m\u001b[36m179\u001b[37m3\u001b[36m.\u001b[37m\u001b[36m He\u001b[37m married\u001b[36m José\u001b[37m\u001b[36mph\u001b[37m\u001b[36mine\u001b[37m de\u001b[36m Beau\u001b[37m\u001b[36mh\u001b[37m\u001b[36marn\u001b[37mais\u001b[36m in\u001b[37m\u001b[36m \u001b[37m\u001b[36m179\u001b[37m6.\u001b[36m Napoleon\u001b[37m\u001b[36m's\u001b[37m\u001b[36m military\u001b[37m successes, including\u001b[36m the\u001b[37m\u001b[36m Italian\u001b[37m Campaign\u001b[36m,\u001b[37m led\u001b[36m to\u001b[37m\u001b[36m his\u001b[37m promotion\u001b[36m to\u001b[37m\u001b[36m General\u001b[37m\u001b[36m-in\u001b[37m-Ch\u001b[36mief\u001b[37m\u001b[36m of\u001b[37m\u001b[36m the\u001b[37m Army\u001b[36m of\u001b[37m\u001b[36m Italy\u001b[37m\u001b[36m.\u001b[37m He returned\u001b[36m to\u001b[37m\u001b[36m Paris\u001b[37m and over\u001b[36mth\u001b[37m\u001b[36mrew\u001b[37m\u001b[36m the\u001b[37m French\u001b[36m Directory\u001b[37m\u001b[36m,\u001b[37m seizing\u001b[36m power\u001b[37m\u001b[36m in\u001b[37m the coup d\u001b[36m'ét\u001b[37m\u001b[36mat\u001b[37m\u001b[36m of\u001b[37m \u001b[36m18\u001b[37m\u001b[36m Br\u001b[37m\u001b[36mum\u001b[37maire\u001b[36m (\u001b[37m\u001b[36mNovember\u001b[37m\u001b[36m \u001b[37m9\u001b[36m,\u001b[37m\u001b[36m \u001b[37m\u001b[36m179\u001b[37m9\u001b[36m).\n",
      "\n",
      "\u001b[37m\u001b[36m**\u001b[37m\u001b[36mCons\u001b[37mulate\u001b[36m and\u001b[37m\u001b[36m Empire\u001b[37m\u001b[36m (\u001b[37m180\u001b[36m4\u001b[37m\u001b[36m-\u001b[37m\u001b[36m181\u001b[37m5\u001b[36m)**\u001b[37m\u001b[36m\n",
      "\n",
      "\u001b[37m\u001b[36mN\u001b[37map\u001b[36moleon\u001b[37m was appointed Cons\u001b[36mul\u001b[37m\u001b[36m for\u001b[37m\u001b[36m Life\u001b[37m in\u001b[36m \u001b[37m\u001b[36m180\u001b[37m\u001b[36m2\u001b[37m and\n",
      "\n",
      "encoded   37 tokens in    4.655 seconds, speed:    7.949 t/s\n",
      "decoded  254 tokens in   98.596 seconds, speed:    2.576 t/s\n",
      "\n",
      "n_draft   = 3\n",
      "n_predict = 254\n",
      "n_drafted = 243\n",
      "n_accept  = 172\n",
      "accept    = 70.782%\n",
      "\n",
      "draft:\n",
      "\n",
      "llama_print_timings:        load time =    3627.46 ms\n",
      "llama_print_timings:      sample time =    1389.51 ms /     1 runs   ( 1389.51 ms per token,     0.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =   93357.59 ms /   198 tokens (  471.50 ms per token,     2.12 tokens per second)\n",
      "llama_print_timings:        eval time =    2549.32 ms /   162 runs   (   15.74 ms per token,    63.55 tokens per second)\n",
      "llama_print_timings:       total time =  103308.56 ms /   360 tokens\n",
      "\n",
      "target:\n",
      "\n",
      "llama_print_timings:        load time =   24563.41 ms\n",
      "llama_print_timings:      sample time =      26.38 ms /   254 runs   (    0.10 ms per token,  9629.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   97127.71 ms /   361 tokens (  269.05 ms per token,     3.72 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  107208.95 ms /   362 tokens\n",
      "\n",
      "\n",
      "CPU times: user 1.24 s, sys: 215 ms, total: 1.46 s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!./llama.cpp/speculative -ngl 45 -ngld 99 --color -m  {larger_model_path} --model-draft {smaller_model_path} -e --temp -1 -p \"{prompt}\" --temp 0.0 -n 250 -s 1 --top-k 1 --draft 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "encoded   37 tokens in    4.655 seconds, speed:    7.949 t/s\n",
    "decoded  254 tokens in   98.596 seconds, speed:    2.576 t/s\n",
    "\n",
    "n_draft   = 3\n",
    "n_predict = 254\n",
    "n_drafted = 243\n",
    "n_accept  = 172\n",
    "accept    = 70.782%\n",
    "```\n",
    "\n",
    "### Speculative decoding version runs 2.576 tokens/seconds with a 70.782% of accepted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up the results\n",
    "\n",
    "The speculative sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                              | GPU Layers offload N | Total Tokens/seconds                                                                                                                                                                       | Gain %  |\n",
    "|--------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------|-----|\n",
    "| Meta-Llama-3-70B-Instruct-Q3_K_S.gguf | 45/80 | 2.01 | - |\n",
    "| Meta-Llama-3-8B-Instruct.Q3_K_L.gguf  | 32/32  | 87.02 | - |\n",
    "| Speculative Sampling   | 45/80 (Model) and 32/32 (Draft Model) | 2.58 | 28% |\n",
    "\n",
    "\n",
    "Speculative sampling has allowed us to sacrifice some VRAM (to load and run the draft model) to achieve a 28% increase in the number of tokens/second in output. \n",
    "\n",
    "Was it worth it? It depends on you and your scenario.\n",
    "\n",
    "Usually if you have more VRAM then what you need to load a Target Model, you should use this technique to sacrifice some VRAM to speed up you inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
