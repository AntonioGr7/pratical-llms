{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Serving LLM for multiple users using SGLang\n",
    "\n",
    "Model like LLama 3.1 and gemma-2 has made it possible to deploy your own LLM on your own machine at a very low cost without relying on a SaaS service. However, putting an LLM into production to serve a large number of users simultaneously is not straightforward. To achieve this, various fast serving frameworks implement best practices, allowing us to do so with great simplicity.\n",
    "\n",
    "Some examples of these features are:\n",
    "\n",
    "- Dynamic Batching: Efficiently group incoming requests into batches to maximize GPU utilization. This involves strategies to handle variable sequence lengths and minimize padding overhead.\n",
    "- Request Queuing and Prioritization: Implement a robust queuing system to manage incoming requests, potentially with prioritization mechanisms for different use cases or users.\n",
    "- Concurrency and Parallelism: Leverage multi-threading and asynchronous operations to handle multiple requests concurrently, maximizing hardware utilization.\n",
    "- Optimized Kernel Execution\n",
    "- Efficient Memory Allocation and Deallocation: Minimize memory fragmentation and overhead associated with memory management.\n",
    "- Model Parameter Sharding: Distribute model parameters across multiple GPUs or devices to handle large models that exceed the memory capacity of a single device.\n",
    "- Memory-Efficient Attention Mechanisms: Implement optimized attention mechanisms that reduce memory consumption, especially for long sequences.\n",
    "- Optimized KV Caching\n",
    "and so on.\n",
    "\n",
    "There are several excellent alternatives such as TGI (from Huggingface), the popular vLLM, TensorRT-LLM, and the latest NVIDIA NIM.\n",
    "\n",
    "Today, I will show how to deploy an LLM to serve 64 users or more simultaneously (potentially in parallel) while maintaining high performance using SGLang."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Requirements\n",
    "\n",
    "To make it work, however, you will need a GPU with Compute Capabilities >= 8.9 (the number of features actually implemented). Only the latest GPUs on the market fall into this category, such as the NVIDIA H100, NVIDIA L4, NVIDIA L40, RTX 6000 and GTX 4090."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (24.2)\n",
      "Requirement already satisfied: sglang[all] in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.2.10)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (4.66.4)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->sglang[all]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->sglang[all]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->sglang[all]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->sglang[all]) (2024.7.4)\n",
      "Requirement already satisfied: anthropic>=0.20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (0.32.0)\n",
      "Requirement already satisfied: litellm>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (1.43.0)\n",
      "Requirement already satisfied: openai>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (1.39.0)\n",
      "Requirement already satisfied: tiktoken in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (0.7.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (3.9.5)\n",
      "Requirement already satisfied: fastapi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (0.111.0)\n",
      "Requirement already satisfied: hf-transfer in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (0.1.8)\n",
      "Requirement already satisfied: huggingface-hub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (0.24.5)\n",
      "Requirement already satisfied: interegular in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (0.3.3)\n",
      "Requirement already satisfied: jsonlines in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (4.0.0)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (24.1)\n",
      "Requirement already satisfied: pillow in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (10.4.0)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (6.0.0)\n",
      "Requirement already satisfied: pydantic in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (2.8.2)\n",
      "Requirement already satisfied: python-multipart in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (0.0.9)\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (2.3.1)\n",
      "Requirement already satisfied: uvicorn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (0.30.1)\n",
      "Requirement already satisfied: uvloop in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (0.19.0)\n",
      "Requirement already satisfied: zmq in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (0.0.0)\n",
      "Requirement already satisfied: vllm==0.5.3.post1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (0.5.3.post1)\n",
      "Requirement already satisfied: outlines>=0.0.44 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sglang[all]) (0.0.46)\n",
      "Requirement already satisfied: cmake>=3.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (3.30.2)\n",
      "Requirement already satisfied: ninja in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (1.11.1.1)\n",
      "Requirement already satisfied: sentencepiece in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (0.2.0)\n",
      "Requirement already satisfied: py-cpuinfo in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (9.0.0)\n",
      "Requirement already satisfied: transformers>=4.42.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (4.43.4)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (0.19.1)\n",
      "Requirement already satisfied: prometheus-client>=0.18.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (0.20.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (7.0.0)\n",
      "Requirement already satisfied: lm-format-enforcer==0.10.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (0.10.3)\n",
      "Requirement already satisfied: typing-extensions in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (4.12.2)\n",
      "Requirement already satisfied: filelock>=3.10.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (3.15.4)\n",
      "Requirement already satisfied: pyzmq in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (26.0.3)\n",
      "Requirement already satisfied: ray>=2.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (2.34.0)\n",
      "Requirement already satisfied: nvidia-ml-py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (12.555.43)\n",
      "Requirement already satisfied: torchvision==0.18.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (0.18.1)\n",
      "Requirement already satisfied: xformers==0.0.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (0.0.27)\n",
      "Requirement already satisfied: vllm-flash-attn==2.5.9.post1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from vllm==0.5.3.post1->sglang[all]) (2.5.9.post1)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (1.13.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->sglang[all]) (2.3.1)\n",
      "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm-format-enforcer==0.10.3->vllm==0.5.3.post1->sglang[all]) (6.0.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->sglang[all]) (12.5.82)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anthropic>=0.20.0->sglang[all]) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anthropic>=0.20.0->sglang[all]) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anthropic>=0.20.0->sglang[all]) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anthropic>=0.20.0->sglang[all]) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anthropic>=0.20.0->sglang[all]) (1.3.1)\n",
      "Requirement already satisfied: click in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from litellm>=1.0.0->sglang[all]) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from litellm>=1.0.0->sglang[all]) (8.2.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from litellm>=1.0.0->sglang[all]) (4.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from litellm>=1.0.0->sglang[all]) (1.0.1)\n",
      "Requirement already satisfied: lark in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from outlines>=0.0.44->sglang[all]) (1.1.9)\n",
      "Requirement already satisfied: nest-asyncio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from outlines>=0.0.44->sglang[all]) (1.6.0)\n",
      "Requirement already satisfied: cloudpickle in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from outlines>=0.0.44->sglang[all]) (3.0.0)\n",
      "Requirement already satisfied: diskcache in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from outlines>=0.0.44->sglang[all]) (5.6.3)\n",
      "Requirement already satisfied: numba in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from outlines>=0.0.44->sglang[all]) (0.60.0)\n",
      "Requirement already satisfied: referencing in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from outlines>=0.0.44->sglang[all]) (0.35.1)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from outlines>=0.0.44->sglang[all]) (2.20.0)\n",
      "Requirement already satisfied: pycountry in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from outlines>=0.0.44->sglang[all]) (24.6.1)\n",
      "Requirement already satisfied: pyairports in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from outlines>=0.0.44->sglang[all]) (2.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic->sglang[all]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic->sglang[all]) (2.20.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tiktoken->sglang[all]) (2024.7.24)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->sglang[all]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->sglang[all]) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->sglang[all]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->sglang[all]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->sglang[all]) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->sglang[all]) (4.0.3)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi->sglang[all]) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi->sglang[all]) (0.0.4)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi->sglang[all]) (5.10.0)\n",
      "Requirement already satisfied: orjson>=3.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi->sglang[all]) (3.10.6)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi->sglang[all]) (2.2.0)\n",
      "Requirement already satisfied: h11>=0.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn->sglang[all]) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic>=0.20.0->sglang[all]) (1.2.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->sglang[all]) (2.6.1)\n",
      "Requirement already satisfied: typer>=0.12.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi-cli>=0.0.2->fastapi->sglang[all]) (0.12.3)\n",
      "Requirement already satisfied: httpcore==1.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic>=0.20.0->sglang[all]) (1.0.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from importlib-metadata>=6.8.0->litellm>=1.0.0->sglang[all]) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch->sglang[all]) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.0.0->sglang[all]) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.0.0->sglang[all]) (0.19.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ray>=2.9->vllm==0.5.3.post1->sglang[all]) (1.0.8)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ray>=2.9->vllm==0.5.3.post1->sglang[all]) (4.23.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.42.4->vllm==0.5.3.post1->sglang[all]) (0.4.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn[standard]->vllm==0.5.3.post1->sglang[all]) (0.6.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn[standard]->vllm==0.5.3.post1->sglang[all]) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn[standard]->vllm==0.5.3.post1->sglang[all]) (12.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->outlines>=0.0.44->sglang[all]) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->outlines>=0.0.44->sglang[all]) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->outlines>=0.0.44->sglang[all]) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->outlines>=0.0.44->sglang[all]) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->outlines>=0.0.44->sglang[all]) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->outlines>=0.0.44->sglang[all]) (0.70.16)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from numba->outlines>=0.0.44->sglang[all]) (0.43.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch->sglang[all]) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->sglang[all]) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->sglang[all]) (13.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets->outlines>=0.0.44->sglang[all]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets->outlines>=0.0.44->sglang[all]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets->outlines>=0.0.44->sglang[all]) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->outlines>=0.0.44->sglang[all]) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->sglang[all]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->sglang[all]) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->sglang[all]) (0.1.2)\n",
      "Looking in indexes: https://flashinfer.ai/whl/cu121/torch2.3/\n",
      "Requirement already satisfied: flashinfer in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.3+cu121torch2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install \"sglang[all]\"\n",
    "!pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.3/ # Install FlashInfer CUDA kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the server "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following `code` in your shell. \n",
    "\n",
    "### Replace [HF_TOKEN] with a personal HF token with reading permissions\n",
    "\n",
    "This step will require some time to download the docker image and the model from Hugginface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code> docker run --gpus all \\\n",
    "    -p 30000:30000 \\\n",
    "    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "    --env \"[HF_TOKEN]\" \\\n",
    "    --ipc=host \\\n",
    "    lmsysorg/sglang:latest \\\n",
    "    python3 -m sglang.launch_server --model-path neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic --host 0.0.0.0 --port 30000 --mem-fraction-static 0.7\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see in the console something like this:\n",
    "\n",
    "\n",
    "![Tux, the Linux mascot](images/server_fired.PNG)\n",
    "\n",
    "If you see this the server is up and running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the LLM server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import threading\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClient():\n",
    "    def __init__(self):\n",
    "        self.client = openai.Client(base_url=\"http://127.0.0.1:30000/v1\", api_key=\"EMPTY\")\n",
    "    def run(self,messages,temperature=0.0,max_tokens=1000):\n",
    "        # Chat completion\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"default\",\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: That's a tough question! Italy is famous for its delicious pizza, and opinions on the best city for pizza can vary depending on personal taste. However, I can give you some popular options:\n",
      "\n",
      "1. **Naples (Napoli)**: Known as the birthplace of pizza, Naples is a must-visit for any pizza lover. The Neapolitan-style pizza, made with fresh ingredients and cooked in a wood-fired oven, is a UNESCO-recognized culinary tradition. Try a classic Margherita pizza at a local pizzeria like Pizzeria Di Matteo or Pizzeria Brandi.\n",
      "2. **Rome**: Rome is another city in Italy famous for its pizza. You'll find many authentic Neapolitan-style pizzerias, like Pizzeria La Montecarlo or Pizzeria Roscioli, serving up delicious pies with fresh ingredients.\n",
      "3. **Milan**: Milan has a unique pizza style, often influenced by the region's rich culinary history\n",
      "Completion Tokens: 200\n",
      "Tokens/Second: 25.85678610374305\n"
     ]
    }
   ],
   "source": [
    "llm = LLMClient()\n",
    "messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "                {\"role\": \"user\", \"content\": \"What is the city in Italy where you can eat the best pizza?\"},\n",
    "            ]\n",
    "start = time.time()\n",
    "r = llm.run(messages=messages,temperature=0.8,max_tokens=200)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Response: {r.choices[0].message.content}\")\n",
    "print(f\"Completion Tokens: {r.usage.completion_tokens}\")\n",
    "print(f\"Tokens/Second: {r.usage.completion_tokens/(end-start)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single request we are getting 25 tokens/seconds. It is not the best but remember that the model is running on a single NVIDIA L4 that is not the best GPU available for FP8 operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how it handles 64 potential users calling the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate a real scenario, we must assume that each user will send a different request from all the others. This is important because sending the same request repeatedly would lead to unrealistic use of the KV cache, resulting in extremely high response times. To do this, we generate N questions that N potential users might ask and evaluate the performance in terms of tokens per second.\n",
    "\n",
    "The steps implemented are as follows:\n",
    "\n",
    "- Generation of N questions in parallel\n",
    "- Sending the N questions to the API, simulating a separation interval of 0.07 seconds between each (sending them all together would actually make the management of dynamic batching simpler for the framework)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(llm_client):\n",
    "    messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that generate random questions\"},\n",
    "                {\"role\": \"user\", \"content\": \"Generate a random question without anything else. I need for a synthetic data. Mix it a lot using 'how', 'why', 'what','which' and so on\"},\n",
    "            ]\n",
    "    questions = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=N) as executor:\n",
    "        # Submit the function call to the executor N times\n",
    "        futures = [executor.submit(llm_client.run,messages,temperature=1.0) for _ in range(N)]\n",
    "        # Collect and print the results as they complete\n",
    "        for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "            response = future.result()\n",
    "            questions.append({\"response\":response.choices[0].message.content,\"completion_tokens\":response.usage.completion_tokens})\n",
    "        return questions\n",
    "\n",
    "def parallel_invoke(llm_client,questions):\n",
    "    messages = []\n",
    "    for q in questions:\n",
    "        messages.append([\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that answer questions\"},\n",
    "                    {\"role\": \"user\", \"content\": q['response']}])\n",
    "    answers = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=N) as executor:\n",
    "        # Submit the function call to the executor N times\n",
    "        futures = [executor.submit(llm_client.run,messages[i],temperature=1.0,max_tokens=200) for i in range(N)]\n",
    "        # Collect and print the results as they complete\n",
    "        for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "            response = future.result()\n",
    "            answers.append({\"response\":response.choices[0].message.content,\"completion_tokens\":response.usage.completion_tokens})\n",
    "        return answers\n",
    "\n",
    "\n",
    "def call_llm(index,llm,messages,max_tokens):\n",
    "    start = time.time()\n",
    "    response = llm.run(messages=messages,temperature=0.0,max_tokens=max_tokens)\n",
    "    end = time.time()\n",
    "    responses[index] = {\"response\":response, \"exec_time\":end-start}\n",
    "    \n",
    "\n",
    "def batch_requests(llm_client,messages,batch_size=1,interval_time=0.07,max_tokens=100):\n",
    "    timers = []\n",
    "    throughputs = []\n",
    "    #total_tokens = 0\n",
    "    for i in range(batch_size):\n",
    "        t=threading.Timer(interval_time*i,call_llm, args=(i,llm_client,messages[i],max_tokens))\n",
    "        timers.append(t)\n",
    "        t.start()\n",
    "    for t in timers:\n",
    "        t.join()\n",
    "    for r in responses:\n",
    "        total_tokens = r['response'].usage.total_tokens\n",
    "        #print(r['response'].choices[0].message.content)\n",
    "        timed = r['exec_time']\n",
    "        throughputs.append(total_tokens/timed)\n",
    "        print(f\"Tokens/Second: {total_tokens/timed}\")\n",
    "    return throughputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's generate the N questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens/second: 539.0938228646794\n"
     ]
    }
   ],
   "source": [
    "N = 64\n",
    "\n",
    "llm = LLMClient()\n",
    "start = time.time()\n",
    "questions = generate_question(llm_client=llm)\n",
    "end = time.time()\n",
    "total_tokens= 0\n",
    "for q in questions:\n",
    "    total_tokens += q['completion_tokens']\n",
    "print(f\"Total tokens/second: {((total_tokens)/(end-start))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a parallel invokation (every request spawn in the same time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Tokens/second: 1101.2147646320898\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "response = parallel_invoke(llm,questions)\n",
    "end = time.time()\n",
    "total_tokens = sum([r['completion_tokens'] for r in response])\n",
    "print(f\"Average Tokens/second: {total_tokens/(end-start)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In parallel we have served 64 parallel request in less then 11 seconds, for a total of 1100 tokens/seconds handled. \n",
    "\n",
    "Not bad at all. Let's see every single user what performance are experiencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's send each question with an interval of 0.07 seconds from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens/Second: 22.481655557363872\n",
      "Tokens/Second: 20.86233051398501\n",
      "Tokens/Second: 21.993210432250052\n",
      "Tokens/Second: 21.862647076802933\n",
      "Tokens/Second: 22.473124052874006\n",
      "Tokens/Second: 22.525694930439993\n",
      "Tokens/Second: 22.76676529303312\n",
      "Tokens/Second: 21.967466851255836\n",
      "Tokens/Second: 22.201399894963224\n",
      "Tokens/Second: 22.34818522383896\n",
      "Tokens/Second: 22.680139911751773\n",
      "Tokens/Second: 22.91284934919259\n",
      "Tokens/Second: 22.787741950817207\n",
      "Tokens/Second: 22.297225837366152\n",
      "Tokens/Second: 22.42831067558483\n",
      "Tokens/Second: 22.11863232244019\n",
      "Tokens/Second: 22.90617508997586\n",
      "Tokens/Second: 23.13758767361235\n",
      "Tokens/Second: 23.206375149178957\n",
      "Tokens/Second: 22.70835953395026\n",
      "Tokens/Second: 23.41402763822864\n",
      "Tokens/Second: 22.582414886834012\n",
      "Tokens/Second: 22.718171640603472\n",
      "Tokens/Second: 22.774653329202827\n",
      "Tokens/Second: 22.546546754490745\n",
      "Tokens/Second: 23.16676206291592\n",
      "Tokens/Second: 23.310038544695598\n",
      "Tokens/Second: 23.645004872443455\n",
      "Tokens/Second: 22.93377239063281\n",
      "Tokens/Second: 23.16824986476582\n",
      "Tokens/Second: 23.144918405332767\n",
      "Tokens/Second: 23.656829758350696\n",
      "Tokens/Second: 23.998555038437463\n",
      "Tokens/Second: 24.143872422264693\n",
      "Tokens/Second: 24.20589206426297\n",
      "Tokens/Second: 23.07695006397932\n",
      "Tokens/Second: 23.581501461637746\n",
      "Tokens/Second: 23.284946849130108\n",
      "Tokens/Second: 23.80787591053049\n",
      "Tokens/Second: 23.853219038811442\n",
      "Tokens/Second: 24.610969688375572\n",
      "Tokens/Second: 24.174644390766677\n",
      "Tokens/Second: 23.69658968085518\n",
      "Tokens/Second: 23.745542290486156\n",
      "Tokens/Second: 24.0902312194509\n",
      "Tokens/Second: 24.260755764196457\n",
      "Tokens/Second: 25.161283786125704\n",
      "Tokens/Second: 24.65509143723775\n",
      "Tokens/Second: 25.58530940369468\n",
      "Tokens/Second: 25.830452170413274\n",
      "Tokens/Second: 25.09513404088457\n",
      "Tokens/Second: 25.53112950624546\n",
      "Tokens/Second: 25.772358690959567\n",
      "Tokens/Second: 26.320385213072623\n",
      "Tokens/Second: 26.404326675405567\n",
      "Tokens/Second: 25.91981313477137\n",
      "Tokens/Second: 26.592002247015177\n",
      "Tokens/Second: 26.28528421436499\n",
      "Tokens/Second: 25.557700502661156\n",
      "Tokens/Second: 26.767916809689805\n",
      "Tokens/Second: 27.14138444822091\n",
      "Tokens/Second: 28.249884077798328\n",
      "Tokens/Second: 29.21600917484692\n",
      "Tokens/Second: 30.774477095938966\n"
     ]
    }
   ],
   "source": [
    "N = 64 # Batch Size\n",
    "responses = [None] * N\n",
    "\n",
    "batch_messages = []\n",
    "for q in questions:\n",
    "    batch_messages.append([\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "                {\"role\": \"user\", \"content\": q['response']}])\n",
    "throughputs = batch_requests(llm,batch_messages,batch_size=N,interval_time=0.07,max_tokens=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are serving 64 users in parallel ensuring a decent throughput to all the users (21 to 29 tokens/second on a single NVIDIA L4)\n",
    "#### Note that the model is not quanitized but it is the FP8 version and SGLang can handle AWQ/FP8/GPTQ/Marlin format, so this numbers can still be improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's test with 128 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 parallel questions generation time: 7.0390636920928955\n",
      "Tokens/Second: 17.286992033894414\n",
      "Tokens/Second: 16.592788713455715\n",
      "Tokens/Second: 16.60591790331349\n",
      "Tokens/Second: 17.191043674119104\n",
      "Tokens/Second: 16.98837166994836\n",
      "Tokens/Second: 16.93018850114843\n",
      "Tokens/Second: 17.546547677695138\n",
      "Tokens/Second: 16.83857269735047\n",
      "Tokens/Second: 16.92173122069925\n",
      "Tokens/Second: 17.148522844716943\n",
      "Tokens/Second: 17.089902459644357\n",
      "Tokens/Second: 17.24558245768498\n",
      "Tokens/Second: 17.334764222814787\n",
      "Tokens/Second: 17.63456014162903\n",
      "Tokens/Second: 16.84011196233023\n",
      "Tokens/Second: 17.125831074694982\n",
      "Tokens/Second: 17.208415311559225\n",
      "Tokens/Second: 17.29401385276943\n",
      "Tokens/Second: 17.58980823644283\n",
      "Tokens/Second: 17.181779590582444\n",
      "Tokens/Second: 17.763938673266328\n",
      "Tokens/Second: 17.63849967158452\n",
      "Tokens/Second: 16.877400247871364\n",
      "Tokens/Second: 17.230581852474334\n",
      "Tokens/Second: 27.032927940112195\n",
      "Tokens/Second: 17.12178138317449\n",
      "Tokens/Second: 17.274694892507316\n",
      "Tokens/Second: 17.29010981763018\n",
      "Tokens/Second: 17.66004961996608\n",
      "Tokens/Second: 17.60374209121929\n",
      "Tokens/Second: 17.11597261313818\n",
      "Tokens/Second: 17.195914966297888\n",
      "Tokens/Second: 17.348084549847023\n",
      "Tokens/Second: 17.433819393522416\n",
      "Tokens/Second: 17.51202615485739\n",
      "Tokens/Second: 17.665742769984746\n",
      "Tokens/Second: 17.75576915339795\n",
      "Tokens/Second: 17.77172302537953\n",
      "Tokens/Second: 17.370871550843734\n",
      "Tokens/Second: 17.452103651950885\n",
      "Tokens/Second: 16.43720698537738\n",
      "Tokens/Second: 17.412789678701618\n",
      "Tokens/Second: 17.778124111868358\n",
      "Tokens/Second: 17.71722905608749\n",
      "Tokens/Second: 17.665835630636757\n",
      "Tokens/Second: 18.028980408609076\n",
      "Tokens/Second: 18.26722461161426\n",
      "Tokens/Second: 17.47034672456436\n",
      "Tokens/Second: 17.691460603549093\n",
      "Tokens/Second: 17.430862403104722\n",
      "Tokens/Second: 17.50766345706006\n",
      "Tokens/Second: 18.01903219175177\n",
      "Tokens/Second: 17.76318048367536\n",
      "Tokens/Second: 17.626570336243667\n",
      "Tokens/Second: 17.786802174192296\n",
      "Tokens/Second: 18.376411293954018\n",
      "Tokens/Second: 16.971210227009394\n",
      "Tokens/Second: 17.944979758046596\n",
      "Tokens/Second: 18.09503992247002\n",
      "Tokens/Second: 18.244427338837678\n",
      "Tokens/Second: 18.27771172839648\n",
      "Tokens/Second: 18.285466643669665\n",
      "Tokens/Second: 17.130733650810654\n",
      "Tokens/Second: 16.641931004320274\n",
      "Tokens/Second: 18.90466551280624\n",
      "Tokens/Second: 18.72712987828542\n",
      "Tokens/Second: 18.236502880097266\n",
      "Tokens/Second: 18.45144837603314\n",
      "Tokens/Second: 18.469025800549613\n",
      "Tokens/Second: 18.50824175372988\n",
      "Tokens/Second: 48.77512082487981\n",
      "Tokens/Second: 16.7812381608653\n",
      "Tokens/Second: 19.00045708156971\n",
      "Tokens/Second: 19.2340080735169\n",
      "Tokens/Second: 17.748030412473053\n",
      "Tokens/Second: 19.272589309514103\n",
      "Tokens/Second: 19.59164951285711\n",
      "Tokens/Second: 19.029233337827048\n",
      "Tokens/Second: 17.16972344668565\n",
      "Tokens/Second: 17.181329315330668\n",
      "Tokens/Second: 17.395777019257913\n",
      "Tokens/Second: 19.618008884493282\n",
      "Tokens/Second: 17.483472652376367\n",
      "Tokens/Second: 17.739479181512092\n",
      "Tokens/Second: 19.619883166447913\n",
      "Tokens/Second: 19.93354170368957\n",
      "Tokens/Second: 18.647814715638848\n",
      "Tokens/Second: 18.154443724835755\n",
      "Tokens/Second: 18.315600659911777\n",
      "Tokens/Second: 18.289160618820315\n",
      "Tokens/Second: 17.645839176165797\n",
      "Tokens/Second: 19.97365048275466\n",
      "Tokens/Second: 18.20206399730917\n",
      "Tokens/Second: 17.86430209425346\n",
      "Tokens/Second: 18.39922641374334\n",
      "Tokens/Second: 18.181678368947956\n",
      "Tokens/Second: 18.428069356464977\n",
      "Tokens/Second: 18.731681423720676\n",
      "Tokens/Second: 20.25054753909422\n",
      "Tokens/Second: 21.010008887315433\n",
      "Tokens/Second: 18.06753870954558\n",
      "Tokens/Second: 18.698115166880473\n",
      "Tokens/Second: 19.171806484623303\n",
      "Tokens/Second: 18.883850025423648\n",
      "Tokens/Second: 18.680304714741677\n",
      "Tokens/Second: 18.64540201398151\n",
      "Tokens/Second: 18.722470789905994\n",
      "Tokens/Second: 19.061774516940968\n",
      "Tokens/Second: 19.2120890360191\n",
      "Tokens/Second: 19.93541172870455\n",
      "Tokens/Second: 18.99235421693967\n",
      "Tokens/Second: 22.474572759696848\n",
      "Tokens/Second: 19.859838912199756\n",
      "Tokens/Second: 19.74500725119519\n",
      "Tokens/Second: 20.25968899794843\n",
      "Tokens/Second: 20.75464804415144\n",
      "Tokens/Second: 20.48763055733429\n",
      "Tokens/Second: 21.486489544372752\n",
      "Tokens/Second: 21.376092760847506\n",
      "Tokens/Second: 22.707931536619522\n",
      "Tokens/Second: 21.49337442444724\n",
      "Tokens/Second: 22.28136731311357\n",
      "Tokens/Second: 22.392053687442488\n",
      "Tokens/Second: 22.094323682844184\n",
      "Tokens/Second: 23.2985802470317\n",
      "Tokens/Second: 22.354353231639795\n",
      "Tokens/Second: 22.866287613534997\n",
      "Tokens/Second: 27.584783091679014\n"
     ]
    }
   ],
   "source": [
    "N = 128 # Batch Size\n",
    "responses = [None] * N\n",
    "\n",
    "llm = LLMClient()\n",
    "start = time.time()\n",
    "questions = generate_question(llm_client=llm)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{N} parallel questions generation time: {end-start}\")\n",
    "\n",
    "batch_messages = []\n",
    "for q in questions:\n",
    "    batch_messages.append([\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "                {\"role\": \"user\", \"content\": q['response']}])\n",
    "throughputs = batch_requests(llm,batch_messages,batch_size=N,interval_time=0.07,max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.79558754291914"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([t for t in throughputs])/len(throughputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 128 requests starting with an interval of 0.07 second from one to another served in 20 seconds with an average of 18.5 tokens/seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "SGLang is a fantastic serving framework to garantee a good throughput to all your users in application with multiple users. \n",
    "The framework handle very well a LLama-3.1-8B in FP8 on a single NVIDIA L4 serving 64 parallel users. Increase the number of users will start degrade the performance but will keep the throughput valid for every user. \n",
    "\n",
    "If the users based grow a lot, you will need to add other GPUs or multiple nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
