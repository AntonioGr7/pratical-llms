{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff5bd55a4f0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "torch.manual_seed(1230)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_it(start,end):\n",
    "    nano = end-start\n",
    "    return nano/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "max_token = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Precision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 4,423,265,024 bytes\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Print model size\n",
    "print(f\"Model size: {model.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Smith and I am a software engineer. I have been working in the software industry for the past 5 years and have experience in developing web applications using various technologies such as Java, JavaScript, and HTML. I am proficient in using tools such as Git, JIRA, and Slack to manage projects and communicate with team members. I am also skilled in designing and implementing user-friendly interfaces using CSS and HTML. In my free time, I enjoy playing video games, reading books, and spending time with my family and friends. I am passionate about learning new technologies and staying up-to-date with the latest trends in the industry. I am looking forward to working with you and contributing to the development of the project. Thank you for considering my application. Best regards,\n",
      "\n",
      "[Your Name]\n",
      "Seconds: 4.424143006\n",
      "Token/s 40.91187824501349\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "t = time_it(start,end)\n",
    "print(\"Seconds:\",t)\n",
    "print(\"Token/s\",len(outputs[0])/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INT 8 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1,242,749,696 bytes\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "   load_in_8bit=True,\n",
    "   bnb_8bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "print(f\"Model size: {model_8bit.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Smith and I am a software engineer. I have been working on a project for the past 6 months and I am excited to share it with you.\n",
      "\n",
      "The project is a web application that allows users to create and manage their own online portfolios. The portfolios can be customized with images, videos, and text, and users can also add links to their social media profiles. The application also includes a search function that allows users to find specific portfolios based on keywords or tags.\n",
      "\n",
      "The design of the application is clean and modern, with a focus on simplicity and usability. The user interface is intuitive and easy to navigate, with a clear navigation menu and a responsive design that adapts to different screen sizes.\n",
      "\n",
      "The application is built using React, a popular front-end framework that allows us to create reusable components and manage state efficiently. We have also used Redux, a state management library that allows us to manage state in\n",
      "Seconds: 24.980280081\n",
      "Token/s 8.206473239502346\n",
      "CPU times: user 25 s, sys: 5.53 ms, total: 25 s\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model_8bit.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "t = time_it(start,end)\n",
    "print(\"Seconds:\",t)\n",
    "print(\"Token/s\",len(outputs[0])/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_8bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INT4 Quantization FP4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 758,307,584 bytes\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "print(f\"Model size: {model_4bit.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John and I am 25 years old. I am a student and I am studying in the University of London. I am a very enthusiastic and motivated person. I am very interested in sports and fitness. I have a passion for fitness and I love to work out. I am a very hardworking person and I am always looking for ways to improve my fitness. I am a very active person and I love to go out and exercise. I am very passionate about fitness and I love to share my knowledge and experience with others. I am a very friendly and outgoing person and I love to meet new people and make new friends. I am a very positive and optimistic person and I always see the best in people. I am a very hardworking person and I always put in the effort to achieve my goals. I am a very determined person and I always have a positive attitude towards life. I am a very creative and artistic person and\n",
      "Seconds: 9.891947199\n",
      "Token/s 20.723927845138913\n",
      "CPU times: user 9.9 s, sys: 0 ns, total: 9.9 s\n",
      "Wall time: 9.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model_4bit.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "t = time_it(start,end)\n",
    "print(\"Seconds:\",t)\n",
    "print(\"Token/s\",len(outputs[0])/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INT4 Quantization NF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 758,307,584 bytes\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "print(f\"Model size: {model_4bit.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Smith and I am a student at the University of XYZ. I am currently enrolled in the Bachelor of Science in Computer Science program. I am currently taking 12 credits and have completed 10 credits. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project\n",
      "Seconds: 10.365734959\n",
      "Token/s 19.776697051472432\n",
      "CPU times: user 10.4 s, sys: 2.19 ms, total: 10.4 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model_4bit.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "t = time_it(start,end)\n",
    "print(\"Seconds:\",t)\n",
    "print(\"Token/s\",len(outputs[0])/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested 4Bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 758,307,584 bytes\n"
     ]
    }
   ],
   "source": [
    "double_quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "print(f\"Model size: {model_4bit.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Smith and I am a student at the University of XYZ. I am currently enrolled in the Bachelor of Science in Computer Science program. I am currently taking 12 credits and have completed 10 credits. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project\n",
      "Seconds: 10.141085694\n",
      "Token/s 20.214798117847362\n",
      "CPU times: user 10.1 s, sys: 10.5 ms, total: 10.1 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model_4bit.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "t = time_it(start,end)\n",
    "print(\"Seconds:\",t)\n",
    "print(\"Token/s\",len(outputs[0])/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Quantization feature together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 758,307,584 bytes\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "print(f\"Model size: {model_4bit.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Smith and I am a student at the University of XYZ. I am currently enrolled in the Bachelor of Science in Computer Science program. I am currently taking 12 credits and have completed 10 credits. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project\n",
      "Seconds: 9.325512288\n",
      "Token/s 21.982706544046106\n",
      "CPU times: user 9.33 s, sys: 2.92 ms, total: 9.33 s\n",
      "Wall time: 9.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model_4bit.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "t = time_it(start,end)\n",
    "print(\"Seconds:\",t)\n",
    "print(\"Token/s\",len(outputs[0])/t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Quant | Memory (GB) | Inference (Tokens/s) |\n",
    "| ------ | -------- | ------- |\n",
    "| Full Precision | 4,4 | 40.91 |\n",
    "| 8bit | 1,2 | 8.2 |\n",
    "| 4 bit FP4 | 0.750 | 20.72 | \n",
    "| 4 bit Normal Float 4 | 0.750 |19.77 |\n",
    "| Nested 4 bit | 0.758 | 20.21 | \n",
    "| All together | 0.750 | 21.98 | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
