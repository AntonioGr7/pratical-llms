{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install auto-gptq --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "out_dir = model_id + \"-GPTQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    damp_percent=0.01,\n",
    "    desc_act=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaGPTQForCausalLM(\n",
       "  (model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32000, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-21): 22 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaSdpaAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config,torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (252167 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "data = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.00001-of-01024.json.gz\", split=f\"train[:{n_samples*5}]\")\n",
    "tokenized_data = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')\n",
    "\n",
    "# Format tokenized examples\n",
    "examples_ids = []\n",
    "for _ in range(n_samples):\n",
    "    i = random.randint(0, tokenized_data.input_ids.shape[1] - tokenizer.model_max_length - 1)\n",
    "    j = i + tokenizer.model_max_length\n",
    "    input_ids = tokenized_data.input_ids[:, i:j]\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    examples_ids.append({'input_ids': input_ids, 'attention_mask': attention_mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/22...\n",
      "INFO - Start quantizing layer 2/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/22...\n",
      "INFO - Start quantizing layer 3/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/22...\n",
      "INFO - Start quantizing layer 4/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/22...\n",
      "INFO - Start quantizing layer 5/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/22...\n",
      "INFO - Start quantizing layer 6/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/22...\n",
      "INFO - Start quantizing layer 7/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/22...\n",
      "INFO - Start quantizing layer 8/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/22...\n",
      "INFO - Start quantizing layer 9/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/22...\n",
      "INFO - Start quantizing layer 10/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/22...\n",
      "INFO - Start quantizing layer 11/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/22...\n",
      "INFO - Start quantizing layer 12/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/22...\n",
      "INFO - Start quantizing layer 13/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/22...\n",
      "INFO - Start quantizing layer 14/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/22...\n",
      "INFO - Start quantizing layer 15/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/22...\n",
      "INFO - Start quantizing layer 16/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/22...\n",
      "INFO - Start quantizing layer 17/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/22...\n",
      "INFO - Start quantizing layer 18/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/22...\n",
      "INFO - Start quantizing layer 19/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/22...\n",
      "INFO - Start quantizing layer 20/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/22...\n",
      "INFO - Start quantizing layer 21/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/22...\n",
      "INFO - Start quantizing layer 22/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/22...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 16s, sys: 1min 19s, total: 8min 35s\n",
      "Wall time: 7min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Quantize with GPTQ\n",
    "model.quantize(\n",
    "    examples_ids,\n",
    "    batch_size=1,\n",
    "    use_triton=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TinyLlama/TinyLlama-1.1B-Chat-v1.0-GPTQ/tokenizer_config.json',\n",
       " 'TinyLlama/TinyLlama-1.1B-Chat-v1.0-GPTQ/special_tokens_map.json',\n",
       " 'TinyLlama/TinyLlama-1.1B-Chat-v1.0-GPTQ/tokenizer.model',\n",
       " 'TinyLlama/TinyLlama-1.1B-Chat-v1.0-GPTQ/added_tokens.json',\n",
       " 'TinyLlama/TinyLlama-1.1B-Chat-v1.0-GPTQ/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_quantized(out_dir, use_safetensors=True)\n",
    "tokenizer.save_pretrained(out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def time_it(start,end):\n",
    "    nano = end-start\n",
    "    return nano/1e9\n",
    "\n",
    "max_token = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "INFO - The layer lm_head is not quantized.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Reload model and tokenizer\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    out_dir,\n",
    "    device=device,\n",
    "    use_triton=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Smith and I am a student at the University of California, Berkeley. I am a junior majoring in Computer Science. I am currently working on a project called \"The Internet of Things\" which is a project that I have been working on for the past two years. The project is focused on creating a platform that allows people to connect their devices to the internet. The platform will allow people to control their devices remotely, and it will also allow people to monitor their devices in real\n",
      "Seconds: 18.003240955\n",
      "Token/s 5.832283212919982\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "t = time_it(start,end)\n",
    "print(\"Seconds:\",t)\n",
    "print(\"Token/s\",len(outputs[0])/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 779,596,544 bytes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model size: {model.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
