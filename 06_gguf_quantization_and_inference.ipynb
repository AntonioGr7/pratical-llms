{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloning necessary repo and installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
    "!pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "!sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = \"[YOUR HF TOKEN HERE]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model_name = model_id.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TinyLlama-1.1B-Chat-v1.0'...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/{model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file TinyLlama-1.1B-Chat-v1.0/model.safetensors\n",
      "params = Params(n_vocab=32000, n_embd=2048, n_layer=22, n_ctx=2048, n_ff=5632, n_head=32, n_head_kv=4, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('TinyLlama-1.1B-Chat-v1.0'))\n",
      "Loaded vocab file PosixPath('TinyLlama-1.1B-Chat-v1.0/tokenizer.model'), type 'spm'\n",
      "Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0, 'pad': 2}, add special tokens unset>\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "lm_head.weight                                   -> output.weight                            | BF16   | [32000, 2048]\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | BF16   | [32000, 2048]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.norm.weight                                -> output_norm.weight                       | BF16   | [2048]\n",
      "Writing TinyLlama-1.1B-Chat-v1.0/tinyllama-1.1b-chat-v1.0.fp16.bin, format 1\n",
      "Ignoring added_tokens.json since model matches vocab size without it.\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting special token type pad to 2\n",
      "gguf: Setting chat_template to {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "[  1/201] Writing tensor output.weight                          | size  32000 x   2048  | type F16  | T+   1\n",
      "[  2/201] Writing tensor token_embd.weight                      | size  32000 x   2048  | type F16  | T+   1\n",
      "[  3/201] Writing tensor blk.0.attn_norm.weight                 | size   2048           | type F32  | T+   1\n",
      "[  4/201] Writing tensor blk.0.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   1\n",
      "[  5/201] Writing tensor blk.0.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   1\n",
      "[  6/201] Writing tensor blk.0.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   1\n",
      "[  7/201] Writing tensor blk.0.ffn_norm.weight                  | size   2048           | type F32  | T+   1\n",
      "[  8/201] Writing tensor blk.0.attn_k.weight                    | size    256 x   2048  | type F16  | T+   1\n",
      "[  9/201] Writing tensor blk.0.attn_output.weight               | size   2048 x   2048  | type F16  | T+   1\n",
      "[ 10/201] Writing tensor blk.0.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   1\n",
      "[ 11/201] Writing tensor blk.0.attn_v.weight                    | size    256 x   2048  | type F16  | T+   1\n",
      "[ 12/201] Writing tensor blk.1.attn_norm.weight                 | size   2048           | type F32  | T+   1\n",
      "[ 13/201] Writing tensor blk.1.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   1\n",
      "[ 14/201] Writing tensor blk.1.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   1\n",
      "[ 15/201] Writing tensor blk.1.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   1\n",
      "[ 16/201] Writing tensor blk.1.ffn_norm.weight                  | size   2048           | type F32  | T+   1\n",
      "[ 17/201] Writing tensor blk.1.attn_k.weight                    | size    256 x   2048  | type F16  | T+   1\n",
      "[ 18/201] Writing tensor blk.1.attn_output.weight               | size   2048 x   2048  | type F16  | T+   1\n",
      "[ 19/201] Writing tensor blk.1.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   1\n",
      "[ 20/201] Writing tensor blk.1.attn_v.weight                    | size    256 x   2048  | type F16  | T+   1\n",
      "[ 21/201] Writing tensor blk.10.attn_norm.weight                | size   2048           | type F32  | T+   1\n",
      "[ 22/201] Writing tensor blk.10.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   1\n",
      "[ 23/201] Writing tensor blk.10.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   1\n",
      "[ 24/201] Writing tensor blk.10.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 25/201] Writing tensor blk.10.ffn_norm.weight                 | size   2048           | type F32  | T+   2\n",
      "[ 26/201] Writing tensor blk.10.attn_k.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 27/201] Writing tensor blk.10.attn_output.weight              | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 28/201] Writing tensor blk.10.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 29/201] Writing tensor blk.10.attn_v.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 30/201] Writing tensor blk.11.attn_norm.weight                | size   2048           | type F32  | T+   2\n",
      "[ 31/201] Writing tensor blk.11.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   2\n",
      "[ 32/201] Writing tensor blk.11.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 33/201] Writing tensor blk.11.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 34/201] Writing tensor blk.11.ffn_norm.weight                 | size   2048           | type F32  | T+   2\n",
      "[ 35/201] Writing tensor blk.11.attn_k.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 36/201] Writing tensor blk.11.attn_output.weight              | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 37/201] Writing tensor blk.11.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 38/201] Writing tensor blk.11.attn_v.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 39/201] Writing tensor blk.12.attn_norm.weight                | size   2048           | type F32  | T+   2\n",
      "[ 40/201] Writing tensor blk.12.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   2\n",
      "[ 41/201] Writing tensor blk.12.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 42/201] Writing tensor blk.12.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 43/201] Writing tensor blk.12.ffn_norm.weight                 | size   2048           | type F32  | T+   2\n",
      "[ 44/201] Writing tensor blk.12.attn_k.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 45/201] Writing tensor blk.12.attn_output.weight              | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 46/201] Writing tensor blk.12.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 47/201] Writing tensor blk.12.attn_v.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 48/201] Writing tensor blk.13.attn_norm.weight                | size   2048           | type F32  | T+   2\n",
      "[ 49/201] Writing tensor blk.13.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   2\n",
      "[ 50/201] Writing tensor blk.13.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 51/201] Writing tensor blk.13.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 52/201] Writing tensor blk.13.ffn_norm.weight                 | size   2048           | type F32  | T+   2\n",
      "[ 53/201] Writing tensor blk.13.attn_k.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 54/201] Writing tensor blk.13.attn_output.weight              | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 55/201] Writing tensor blk.13.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 56/201] Writing tensor blk.13.attn_v.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 57/201] Writing tensor blk.14.attn_norm.weight                | size   2048           | type F32  | T+   2\n",
      "[ 58/201] Writing tensor blk.14.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   3\n",
      "[ 59/201] Writing tensor blk.14.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   3\n",
      "[ 60/201] Writing tensor blk.14.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   3\n",
      "[ 61/201] Writing tensor blk.14.ffn_norm.weight                 | size   2048           | type F32  | T+   3\n",
      "[ 62/201] Writing tensor blk.14.attn_k.weight                   | size    256 x   2048  | type F16  | T+   3\n",
      "[ 63/201] Writing tensor blk.14.attn_output.weight              | size   2048 x   2048  | type F16  | T+   3\n",
      "[ 64/201] Writing tensor blk.14.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   3\n",
      "[ 65/201] Writing tensor blk.14.attn_v.weight                   | size    256 x   2048  | type F16  | T+   3\n",
      "[ 66/201] Writing tensor blk.15.attn_norm.weight                | size   2048           | type F32  | T+   3\n",
      "[ 67/201] Writing tensor blk.15.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   3\n",
      "[ 68/201] Writing tensor blk.15.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   3\n",
      "[ 69/201] Writing tensor blk.15.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   3\n",
      "[ 70/201] Writing tensor blk.15.ffn_norm.weight                 | size   2048           | type F32  | T+   3\n",
      "[ 71/201] Writing tensor blk.15.attn_k.weight                   | size    256 x   2048  | type F16  | T+   3\n",
      "[ 72/201] Writing tensor blk.15.attn_output.weight              | size   2048 x   2048  | type F16  | T+   3\n",
      "[ 73/201] Writing tensor blk.15.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   3\n",
      "[ 74/201] Writing tensor blk.15.attn_v.weight                   | size    256 x   2048  | type F16  | T+   3\n",
      "[ 75/201] Writing tensor blk.16.attn_norm.weight                | size   2048           | type F32  | T+   3\n",
      "[ 76/201] Writing tensor blk.16.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   3\n",
      "[ 77/201] Writing tensor blk.16.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   3\n",
      "[ 78/201] Writing tensor blk.16.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   3\n",
      "[ 79/201] Writing tensor blk.16.ffn_norm.weight                 | size   2048           | type F32  | T+   3\n",
      "[ 80/201] Writing tensor blk.16.attn_k.weight                   | size    256 x   2048  | type F16  | T+   3\n",
      "[ 81/201] Writing tensor blk.16.attn_output.weight              | size   2048 x   2048  | type F16  | T+   3\n",
      "[ 82/201] Writing tensor blk.16.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   3\n",
      "[ 83/201] Writing tensor blk.16.attn_v.weight                   | size    256 x   2048  | type F16  | T+   3\n",
      "[ 84/201] Writing tensor blk.17.attn_norm.weight                | size   2048           | type F32  | T+   3\n",
      "[ 85/201] Writing tensor blk.17.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   4\n",
      "[ 86/201] Writing tensor blk.17.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   4\n",
      "[ 87/201] Writing tensor blk.17.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   4\n",
      "[ 88/201] Writing tensor blk.17.ffn_norm.weight                 | size   2048           | type F32  | T+   4\n",
      "[ 89/201] Writing tensor blk.17.attn_k.weight                   | size    256 x   2048  | type F16  | T+   4\n",
      "[ 90/201] Writing tensor blk.17.attn_output.weight              | size   2048 x   2048  | type F16  | T+   4\n",
      "[ 91/201] Writing tensor blk.17.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   4\n",
      "[ 92/201] Writing tensor blk.17.attn_v.weight                   | size    256 x   2048  | type F16  | T+   4\n",
      "[ 93/201] Writing tensor blk.18.attn_norm.weight                | size   2048           | type F32  | T+   4\n",
      "[ 94/201] Writing tensor blk.18.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   4\n",
      "[ 95/201] Writing tensor blk.18.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   4\n",
      "[ 96/201] Writing tensor blk.18.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   4\n",
      "[ 97/201] Writing tensor blk.18.ffn_norm.weight                 | size   2048           | type F32  | T+   4\n",
      "[ 98/201] Writing tensor blk.18.attn_k.weight                   | size    256 x   2048  | type F16  | T+   4\n",
      "[ 99/201] Writing tensor blk.18.attn_output.weight              | size   2048 x   2048  | type F16  | T+   4\n",
      "[100/201] Writing tensor blk.18.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   4\n",
      "[101/201] Writing tensor blk.18.attn_v.weight                   | size    256 x   2048  | type F16  | T+   4\n",
      "[102/201] Writing tensor blk.19.attn_norm.weight                | size   2048           | type F32  | T+   4\n",
      "[103/201] Writing tensor blk.19.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   4\n",
      "[104/201] Writing tensor blk.19.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   4\n",
      "[105/201] Writing tensor blk.19.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   4\n",
      "[106/201] Writing tensor blk.19.ffn_norm.weight                 | size   2048           | type F32  | T+   4\n",
      "[107/201] Writing tensor blk.19.attn_k.weight                   | size    256 x   2048  | type F16  | T+   4\n",
      "[108/201] Writing tensor blk.19.attn_output.weight              | size   2048 x   2048  | type F16  | T+   4\n",
      "[109/201] Writing tensor blk.19.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   4\n",
      "[110/201] Writing tensor blk.19.attn_v.weight                   | size    256 x   2048  | type F16  | T+   4\n",
      "[111/201] Writing tensor blk.2.attn_norm.weight                 | size   2048           | type F32  | T+   4\n",
      "[112/201] Writing tensor blk.2.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   4\n",
      "[113/201] Writing tensor blk.2.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   4\n",
      "[114/201] Writing tensor blk.2.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   4\n",
      "[115/201] Writing tensor blk.2.ffn_norm.weight                  | size   2048           | type F32  | T+   5\n",
      "[116/201] Writing tensor blk.2.attn_k.weight                    | size    256 x   2048  | type F16  | T+   5\n",
      "[117/201] Writing tensor blk.2.attn_output.weight               | size   2048 x   2048  | type F16  | T+   5\n",
      "[118/201] Writing tensor blk.2.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   5\n",
      "[119/201] Writing tensor blk.2.attn_v.weight                    | size    256 x   2048  | type F16  | T+   5\n",
      "[120/201] Writing tensor blk.20.attn_norm.weight                | size   2048           | type F32  | T+   5\n",
      "[121/201] Writing tensor blk.20.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   5\n",
      "[122/201] Writing tensor blk.20.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   5\n",
      "[123/201] Writing tensor blk.20.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   5\n",
      "[124/201] Writing tensor blk.20.ffn_norm.weight                 | size   2048           | type F32  | T+   5\n",
      "[125/201] Writing tensor blk.20.attn_k.weight                   | size    256 x   2048  | type F16  | T+   5\n",
      "[126/201] Writing tensor blk.20.attn_output.weight              | size   2048 x   2048  | type F16  | T+   5\n",
      "[127/201] Writing tensor blk.20.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   5\n",
      "[128/201] Writing tensor blk.20.attn_v.weight                   | size    256 x   2048  | type F16  | T+   5\n",
      "[129/201] Writing tensor blk.21.attn_norm.weight                | size   2048           | type F32  | T+   5\n",
      "[130/201] Writing tensor blk.21.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   5\n",
      "[131/201] Writing tensor blk.21.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   5\n",
      "[132/201] Writing tensor blk.21.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   5\n",
      "[133/201] Writing tensor blk.21.ffn_norm.weight                 | size   2048           | type F32  | T+   5\n",
      "[134/201] Writing tensor blk.21.attn_k.weight                   | size    256 x   2048  | type F16  | T+   5\n",
      "[135/201] Writing tensor blk.21.attn_output.weight              | size   2048 x   2048  | type F16  | T+   5\n",
      "[136/201] Writing tensor blk.21.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   5\n",
      "[137/201] Writing tensor blk.21.attn_v.weight                   | size    256 x   2048  | type F16  | T+   5\n",
      "[138/201] Writing tensor blk.3.attn_norm.weight                 | size   2048           | type F32  | T+   5\n",
      "[139/201] Writing tensor blk.3.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   5\n",
      "[140/201] Writing tensor blk.3.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   5\n",
      "[141/201] Writing tensor blk.3.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   5\n",
      "[142/201] Writing tensor blk.3.ffn_norm.weight                  | size   2048           | type F32  | T+   5\n",
      "[143/201] Writing tensor blk.3.attn_k.weight                    | size    256 x   2048  | type F16  | T+   5\n",
      "[144/201] Writing tensor blk.3.attn_output.weight               | size   2048 x   2048  | type F16  | T+   5\n",
      "[145/201] Writing tensor blk.3.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   5\n",
      "[146/201] Writing tensor blk.3.attn_v.weight                    | size    256 x   2048  | type F16  | T+   5\n",
      "[147/201] Writing tensor blk.4.attn_norm.weight                 | size   2048           | type F32  | T+   5\n",
      "[148/201] Writing tensor blk.4.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   5\n",
      "[149/201] Writing tensor blk.4.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   5\n",
      "[150/201] Writing tensor blk.4.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   5\n",
      "[151/201] Writing tensor blk.4.ffn_norm.weight                  | size   2048           | type F32  | T+   5\n",
      "[152/201] Writing tensor blk.4.attn_k.weight                    | size    256 x   2048  | type F16  | T+   5\n",
      "[153/201] Writing tensor blk.4.attn_output.weight               | size   2048 x   2048  | type F16  | T+   5\n",
      "[154/201] Writing tensor blk.4.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   5\n",
      "[155/201] Writing tensor blk.4.attn_v.weight                    | size    256 x   2048  | type F16  | T+   5\n",
      "[156/201] Writing tensor blk.5.attn_norm.weight                 | size   2048           | type F32  | T+   5\n",
      "[157/201] Writing tensor blk.5.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   5\n",
      "[158/201] Writing tensor blk.5.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   5\n",
      "[159/201] Writing tensor blk.5.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   5\n",
      "[160/201] Writing tensor blk.5.ffn_norm.weight                  | size   2048           | type F32  | T+   6\n",
      "[161/201] Writing tensor blk.5.attn_k.weight                    | size    256 x   2048  | type F16  | T+   6\n",
      "[162/201] Writing tensor blk.5.attn_output.weight               | size   2048 x   2048  | type F16  | T+   6\n",
      "[163/201] Writing tensor blk.5.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   6\n",
      "[164/201] Writing tensor blk.5.attn_v.weight                    | size    256 x   2048  | type F16  | T+   6\n",
      "[165/201] Writing tensor blk.6.attn_norm.weight                 | size   2048           | type F32  | T+   6\n",
      "[166/201] Writing tensor blk.6.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   6\n",
      "[167/201] Writing tensor blk.6.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   6\n",
      "[168/201] Writing tensor blk.6.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   6\n",
      "[169/201] Writing tensor blk.6.ffn_norm.weight                  | size   2048           | type F32  | T+   6\n",
      "[170/201] Writing tensor blk.6.attn_k.weight                    | size    256 x   2048  | type F16  | T+   6\n",
      "[171/201] Writing tensor blk.6.attn_output.weight               | size   2048 x   2048  | type F16  | T+   6\n",
      "[172/201] Writing tensor blk.6.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   6\n",
      "[173/201] Writing tensor blk.6.attn_v.weight                    | size    256 x   2048  | type F16  | T+   6\n",
      "[174/201] Writing tensor blk.7.attn_norm.weight                 | size   2048           | type F32  | T+   6\n",
      "[175/201] Writing tensor blk.7.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   6\n",
      "[176/201] Writing tensor blk.7.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   6\n",
      "[177/201] Writing tensor blk.7.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   6\n",
      "[178/201] Writing tensor blk.7.ffn_norm.weight                  | size   2048           | type F32  | T+   6\n",
      "[179/201] Writing tensor blk.7.attn_k.weight                    | size    256 x   2048  | type F16  | T+   6\n",
      "[180/201] Writing tensor blk.7.attn_output.weight               | size   2048 x   2048  | type F16  | T+   6\n",
      "[181/201] Writing tensor blk.7.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   6\n",
      "[182/201] Writing tensor blk.7.attn_v.weight                    | size    256 x   2048  | type F16  | T+   6\n",
      "[183/201] Writing tensor blk.8.attn_norm.weight                 | size   2048           | type F32  | T+   6\n",
      "[184/201] Writing tensor blk.8.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   6\n",
      "[185/201] Writing tensor blk.8.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   6\n",
      "[186/201] Writing tensor blk.8.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   6\n",
      "[187/201] Writing tensor blk.8.ffn_norm.weight                  | size   2048           | type F32  | T+   6\n",
      "[188/201] Writing tensor blk.8.attn_k.weight                    | size    256 x   2048  | type F16  | T+   6\n",
      "[189/201] Writing tensor blk.8.attn_output.weight               | size   2048 x   2048  | type F16  | T+   6\n",
      "[190/201] Writing tensor blk.8.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   6\n",
      "[191/201] Writing tensor blk.8.attn_v.weight                    | size    256 x   2048  | type F16  | T+   6\n",
      "[192/201] Writing tensor blk.9.attn_norm.weight                 | size   2048           | type F32  | T+   6\n",
      "[193/201] Writing tensor blk.9.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   6\n",
      "[194/201] Writing tensor blk.9.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   7\n",
      "[195/201] Writing tensor blk.9.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   7\n",
      "[196/201] Writing tensor blk.9.ffn_norm.weight                  | size   2048           | type F32  | T+   7\n",
      "[197/201] Writing tensor blk.9.attn_k.weight                    | size    256 x   2048  | type F16  | T+   7\n",
      "[198/201] Writing tensor blk.9.attn_output.weight               | size   2048 x   2048  | type F16  | T+   7\n",
      "[199/201] Writing tensor blk.9.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   7\n",
      "[200/201] Writing tensor blk.9.attn_v.weight                    | size    256 x   2048  | type F16  | T+   7\n",
      "[201/201] Writing tensor output_norm.weight                     | size   2048           | type F32  | T+   7\n",
      "Wrote TinyLlama-1.1B-Chat-v1.0/tinyllama-1.1b-chat-v1.0.fp16.bin\n"
     ]
    }
   ],
   "source": [
    "fp16 = f\"{model_name}/{model_name.lower()}.fp16.bin\"\n",
    "!python llama.cpp/convert.py {model_name} --outtype f16 --outfile {fp16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 2675 (17e98d4c)\n",
      "main: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\n",
      "main: quantizing 'TinyLlama-1.1B-Chat-v1.0/tinyllama-1.1b-chat-v1.0.fp16.bin' to 'TinyLlama-1.1B-Chat-v1.0/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf' as Q4_K_M\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 201 tensors from TinyLlama-1.1B-Chat-v1.0/tinyllama-1.1b-chat-v1.0.fp16.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type  f16:  156 tensors\n",
      "llama_model_quantize_internal: meta size = 736224 bytes\n",
      "[   1/ 201]                        output.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q6_K .. size =   125.00 MiB ->    51.27 MiB\n",
      "[   2/ 201]                    token_embd.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q4_K .. size =   125.00 MiB ->    35.16 MiB\n",
      "[   3/ 201]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   4/ 201]                blk.0.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[   5/ 201]                blk.0.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[   6/ 201]                  blk.0.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[   7/ 201]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   8/ 201]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[   9/ 201]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  10/ 201]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  11/ 201]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[  12/ 201]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  13/ 201]                blk.1.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[  14/ 201]                blk.1.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  15/ 201]                  blk.1.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  16/ 201]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  17/ 201]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  18/ 201]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  19/ 201]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  20/ 201]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[  21/ 201]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  22/ 201]               blk.10.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  23/ 201]               blk.10.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  24/ 201]                 blk.10.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  25/ 201]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  26/ 201]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  27/ 201]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  28/ 201]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  29/ 201]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  30/ 201]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  31/ 201]               blk.11.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  32/ 201]               blk.11.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  33/ 201]                 blk.11.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  34/ 201]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  35/ 201]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  36/ 201]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  37/ 201]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  38/ 201]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  39/ 201]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  40/ 201]               blk.12.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[  41/ 201]               blk.12.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  42/ 201]                 blk.12.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  43/ 201]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  44/ 201]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  45/ 201]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  46/ 201]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  47/ 201]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[  48/ 201]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  49/ 201]               blk.13.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  50/ 201]               blk.13.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  51/ 201]                 blk.13.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  52/ 201]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  53/ 201]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  54/ 201]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  55/ 201]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  56/ 201]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  57/ 201]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  58/ 201]               blk.14.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  59/ 201]               blk.14.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  60/ 201]                 blk.14.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  61/ 201]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  62/ 201]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  63/ 201]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  64/ 201]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  65/ 201]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  66/ 201]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  67/ 201]               blk.15.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[  68/ 201]               blk.15.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  69/ 201]                 blk.15.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  70/ 201]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  71/ 201]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  72/ 201]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  73/ 201]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  74/ 201]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[  75/ 201]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  76/ 201]               blk.16.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  77/ 201]               blk.16.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  78/ 201]                 blk.16.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  79/ 201]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  80/ 201]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  81/ 201]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  82/ 201]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  83/ 201]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  84/ 201]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  85/ 201]               blk.17.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  86/ 201]               blk.17.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  87/ 201]                 blk.17.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  88/ 201]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  89/ 201]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  90/ 201]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  91/ 201]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  92/ 201]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  93/ 201]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  94/ 201]               blk.18.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[  95/ 201]               blk.18.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  96/ 201]                 blk.18.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[  97/ 201]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  98/ 201]                 blk.18.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[  99/ 201]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 100/ 201]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 101/ 201]                 blk.18.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 102/ 201]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 103/ 201]               blk.19.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 104/ 201]               blk.19.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 105/ 201]                 blk.19.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 106/ 201]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 107/ 201]                 blk.19.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 108/ 201]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 109/ 201]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 110/ 201]                 blk.19.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 111/ 201]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 112/ 201]                blk.2.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 113/ 201]                blk.2.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 114/ 201]                  blk.2.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 115/ 201]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 116/ 201]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 117/ 201]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 118/ 201]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 119/ 201]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 120/ 201]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 121/ 201]               blk.20.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[ 122/ 201]               blk.20.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 123/ 201]                 blk.20.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 124/ 201]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 125/ 201]                 blk.20.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 126/ 201]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 127/ 201]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 128/ 201]                 blk.20.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 129/ 201]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 130/ 201]               blk.21.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 131/ 201]               blk.21.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 132/ 201]                 blk.21.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 133/ 201]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 134/ 201]                 blk.21.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 135/ 201]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 136/ 201]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 137/ 201]                 blk.21.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 138/ 201]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 139/ 201]                blk.3.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 140/ 201]                blk.3.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 141/ 201]                  blk.3.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 142/ 201]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 143/ 201]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 144/ 201]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 145/ 201]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 146/ 201]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 147/ 201]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 148/ 201]                blk.4.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[ 149/ 201]                blk.4.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 150/ 201]                  blk.4.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 151/ 201]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 152/ 201]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 153/ 201]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 154/ 201]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 155/ 201]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 156/ 201]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 157/ 201]                blk.5.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 158/ 201]                blk.5.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 159/ 201]                  blk.5.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 160/ 201]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 161/ 201]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 162/ 201]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 163/ 201]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 164/ 201]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 165/ 201]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 166/ 201]                blk.6.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 167/ 201]                blk.6.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 168/ 201]                  blk.6.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 169/ 201]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 170/ 201]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 171/ 201]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 172/ 201]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 173/ 201]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 174/ 201]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 175/ 201]                blk.7.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[ 176/ 201]                blk.7.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 177/ 201]                  blk.7.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 178/ 201]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 179/ 201]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 180/ 201]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 181/ 201]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 182/ 201]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 183/ 201]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 184/ 201]                blk.8.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[ 185/ 201]                blk.8.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 186/ 201]                  blk.8.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 187/ 201]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 188/ 201]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 189/ 201]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 190/ 201]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 191/ 201]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 192/ 201]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 193/ 201]                blk.9.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[ 194/ 201]                blk.9.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 195/ 201]                  blk.9.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n",
      "[ 196/ 201]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 197/ 201]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
      "[ 198/ 201]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 199/ 201]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 200/ 201]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 201/ 201]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "llama_model_quantize_internal: model size  =  2098.35 MB\n",
      "llama_model_quantize_internal: quant size  =   636.18 MB\n",
      "\n",
      "main: quantize time = 34981.44 ms\n",
      "main:    total time = 34981.44 ms\n",
      "main: build = 2675 (17e98d4c)\n",
      "main: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\n",
      "main: quantizing 'TinyLlama-1.1B-Chat-v1.0/tinyllama-1.1b-chat-v1.0.fp16.bin' to 'TinyLlama-1.1B-Chat-v1.0/tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf' as Q5_K_M\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 201 tensors from TinyLlama-1.1B-Chat-v1.0/tinyllama-1.1b-chat-v1.0.fp16.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type  f16:  156 tensors\n",
      "llama_model_quantize_internal: meta size = 736224 bytes\n",
      "[   1/ 201]                        output.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q6_K .. size =   125.00 MiB ->    51.27 MiB\n",
      "[   2/ 201]                    token_embd.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q5_K .. size =   125.00 MiB ->    42.97 MiB\n",
      "[   3/ 201]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   4/ 201]                blk.0.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[   5/ 201]                blk.0.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[   6/ 201]                  blk.0.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[   7/ 201]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   8/ 201]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[   9/ 201]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  10/ 201]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  11/ 201]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[  12/ 201]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  13/ 201]                blk.1.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[  14/ 201]                blk.1.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  15/ 201]                  blk.1.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  16/ 201]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  17/ 201]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  18/ 201]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  19/ 201]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  20/ 201]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[  21/ 201]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  22/ 201]               blk.10.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  23/ 201]               blk.10.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  24/ 201]                 blk.10.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  25/ 201]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  26/ 201]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  27/ 201]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  28/ 201]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  29/ 201]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  30/ 201]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  31/ 201]               blk.11.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  32/ 201]               blk.11.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  33/ 201]                 blk.11.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  34/ 201]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  35/ 201]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  36/ 201]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  37/ 201]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  38/ 201]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  39/ 201]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  40/ 201]               blk.12.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[  41/ 201]               blk.12.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  42/ 201]                 blk.12.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  43/ 201]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  44/ 201]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  45/ 201]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  46/ 201]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  47/ 201]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[  48/ 201]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  49/ 201]               blk.13.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  50/ 201]               blk.13.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  51/ 201]                 blk.13.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  52/ 201]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  53/ 201]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  54/ 201]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  55/ 201]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  56/ 201]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  57/ 201]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  58/ 201]               blk.14.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  59/ 201]               blk.14.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  60/ 201]                 blk.14.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  61/ 201]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  62/ 201]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  63/ 201]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  64/ 201]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  65/ 201]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  66/ 201]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  67/ 201]               blk.15.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[  68/ 201]               blk.15.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  69/ 201]                 blk.15.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  70/ 201]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  71/ 201]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  72/ 201]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  73/ 201]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  74/ 201]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[  75/ 201]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  76/ 201]               blk.16.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  77/ 201]               blk.16.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  78/ 201]                 blk.16.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  79/ 201]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  80/ 201]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  81/ 201]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  82/ 201]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  83/ 201]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  84/ 201]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  85/ 201]               blk.17.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  86/ 201]               blk.17.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  87/ 201]                 blk.17.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  88/ 201]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  89/ 201]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  90/ 201]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  91/ 201]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  92/ 201]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  93/ 201]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  94/ 201]               blk.18.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[  95/ 201]               blk.18.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  96/ 201]                 blk.18.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[  97/ 201]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  98/ 201]                 blk.18.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[  99/ 201]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 100/ 201]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 101/ 201]                 blk.18.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 102/ 201]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 103/ 201]               blk.19.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 104/ 201]               blk.19.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 105/ 201]                 blk.19.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 106/ 201]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 107/ 201]                 blk.19.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 108/ 201]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 109/ 201]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 110/ 201]                 blk.19.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 111/ 201]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 112/ 201]                blk.2.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 113/ 201]                blk.2.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 114/ 201]                  blk.2.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 115/ 201]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 116/ 201]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 117/ 201]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 118/ 201]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 119/ 201]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 120/ 201]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 121/ 201]               blk.20.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[ 122/ 201]               blk.20.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 123/ 201]                 blk.20.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 124/ 201]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 125/ 201]                 blk.20.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 126/ 201]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 127/ 201]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 128/ 201]                 blk.20.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 129/ 201]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 130/ 201]               blk.21.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 131/ 201]               blk.21.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 132/ 201]                 blk.21.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 133/ 201]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 134/ 201]                 blk.21.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 135/ 201]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 136/ 201]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 137/ 201]                 blk.21.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 138/ 201]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 139/ 201]                blk.3.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 140/ 201]                blk.3.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 141/ 201]                  blk.3.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 142/ 201]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 143/ 201]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 144/ 201]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 145/ 201]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 146/ 201]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 147/ 201]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 148/ 201]                blk.4.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[ 149/ 201]                blk.4.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 150/ 201]                  blk.4.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 151/ 201]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 152/ 201]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 153/ 201]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 154/ 201]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 155/ 201]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 156/ 201]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 157/ 201]                blk.5.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 158/ 201]                blk.5.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 159/ 201]                  blk.5.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 160/ 201]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 161/ 201]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 162/ 201]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 163/ 201]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 164/ 201]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 165/ 201]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 166/ 201]                blk.6.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 167/ 201]                blk.6.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 168/ 201]                  blk.6.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 169/ 201]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 170/ 201]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 171/ 201]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 172/ 201]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 173/ 201]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 174/ 201]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 175/ 201]                blk.7.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[ 176/ 201]                blk.7.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 177/ 201]                  blk.7.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 178/ 201]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 179/ 201]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 180/ 201]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 181/ 201]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 182/ 201]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 183/ 201]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 184/ 201]                blk.8.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[ 185/ 201]                blk.8.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 186/ 201]                  blk.8.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 187/ 201]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 188/ 201]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 189/ 201]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 190/ 201]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 191/ 201]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 192/ 201]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 193/ 201]                blk.9.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n",
      "[ 194/ 201]                blk.9.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 195/ 201]                  blk.9.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q5_K .. size =    22.00 MiB ->     7.56 MiB\n",
      "[ 196/ 201]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 197/ 201]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q5_K .. size =     1.00 MiB ->     0.34 MiB\n",
      "[ 198/ 201]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 199/ 201]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 200/ 201]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
      "[ 201/ 201]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "llama_model_quantize_internal: model size  =  2098.35 MB\n",
      "llama_model_quantize_internal: quant size  =   745.11 MB\n",
      "\n",
      "main: quantize time = 28367.40 ms\n",
      "main:    total time = 28367.40 ms\n"
     ]
    }
   ],
   "source": [
    "QUANTIZATION_METHODS = [\"q4_k_m\", \"q5_k_m\"]\n",
    "\n",
    "for method in QUANTIZATION_METHODS:\n",
    "    qtype = f\"{model_name}/{model_name.lower()}.{method.upper()}.gguf\"\n",
    "    !./llama.cpp/quantize {fp16} {qtype} {method}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At this point you should see in the model_name folder two file .gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf',\n",
       " 'tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "model_list = [file for file in os.listdir(model_name) if \"gguf\" in file]\n",
    "model_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model_name = model_id.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write me a bubble sort in C#:\"\n",
    "gguf_model = \"tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/pratical-llms/TinyLlama-1.1B-Chat-v1.0/tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtype = os.getcwd()+f\"/TinyLlama-1.1B-Chat-v1.0/{gguf_model}\"\n",
    "qtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 2675 (17e98d4c)\n",
      "main: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1713182873\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /teamspace/studios/this_studio/pratical-llms/TinyLlama-1.1B-Chat-v1.0/tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type q5_K:  135 tensors\n",
      "llama_model_loader: - type q6_K:   21 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_layer          = 22\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 5632\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 1.10 B\n",
      "llm_load_print_meta: model size       = 745.11 MiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors: offloading 22 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 23/23 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    42.97 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =   702.14 MiB\n",
      "......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    11.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =    66.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 710\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "\n",
      "system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 2048, n_predict = 128, n_keep = 1\n",
      "\n",
      "\n",
      "\u001b[33m Write me a bubble sort in C#:\u001b[0m\n",
      "\n",
      "using System;\n",
      "\n",
      "class BubbleSort\n",
      "{\n",
      "    public static void Main()\n",
      "    {\n",
      "        int[] arr = { 5, 3, 7, 1, 8, 2, 9, 6, 4 };\n",
      "\n",
      "        // Sort array by swapping adjacent elements\n",
      "        for (int I = 0; I < arr.Length - 1; i++)\n",
      "        {\n",
      "            for (int j = 0; j < arr.Length - I - 1; j++)\n",
      "            {\n",
      "                if (arr[j] > arr\n",
      "llama_print_timings:        load time =    2633.60 ms\n",
      "llama_print_timings:      sample time =       4.94 ms /   128 runs   (    0.04 ms per token, 25937.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =      19.73 ms /    11 tokens (    1.79 ms per token,   557.61 tokens per second)\n",
      "llama_print_timings:        eval time =     762.87 ms /   127 runs   (    6.01 ms per token,   166.48 tokens per second)\n",
      "llama_print_timings:       total time =     834.07 ms /   138 tokens\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It worked and is also super fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run the Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182889,\"level\":\"INFO\",\"function\":\"main\",\"line\":2921,\"msg\":\"build info\",\"build\":2675,\"commit\":\"17e98d4c\"}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182889,\"level\":\"INFO\",\"function\":\"main\",\"line\":2926,\"msg\":\"system info\",\"n_threads\":1,\"n_threads_batch\":1,\"total_threads\":8,\"system_info\":\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \"}\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /teamspace/studios/this_studio/pratical-llms/TinyLlama-1.1B-Chat-v1.0/tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type q5_K:  135 tensors\n",
      "llama_model_loader: - type q6_K:   21 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_layer          = 22\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 5632\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 1.10 B\n",
      "llm_load_print_meta: model size       = 745.11 MiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors: offloading 22 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 23/23 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    42.97 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =   702.14 MiB\n",
      "......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 1\n",
      "llama_new_context_with_model: n_ubatch   = 1\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    11.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.24 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =     0.13 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     0.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 710\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182890,\"level\":\"INFO\",\"function\":\"init\",\"line\":708,\"msg\":\"initializing slots\",\"n_slots\":1}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182890,\"level\":\"INFO\",\"function\":\"init\",\"line\":717,\"msg\":\"new slot\",\"id_slot\":0,\"n_ctx_slot\":512}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182890,\"level\":\"INFO\",\"function\":\"main\",\"line\":3021,\"msg\":\"model loaded\"}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182890,\"level\":\"INFO\",\"function\":\"main\",\"line\":3043,\"msg\":\"chat template\",\"chat_example\":\"<|system|>\\nYou are a helpful assistant<|endoftext|>\\n<|user|>\\nHello<|endoftext|>\\n<|assistant|>\\nHi there<|endoftext|>\\n<|user|>\\nHow are you?<|endoftext|>\\n<|assistant|>\\n\",\"built_in\":true}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182890,\"level\":\"INFO\",\"function\":\"main\",\"line\":3774,\"msg\":\"HTTP server listening\",\"n_threads_http\":\"1\",\"port\":\"8080\",\"hostname\":\"127.0.0.1\"}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182890,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1786,\"msg\":\"all slots are idle\"}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"launch_slot_with_task\",\"line\":1037,\"msg\":\"slot is processing task\",\"id_slot\":0,\"id_task\":0}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":0,\"p0\":0}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":0,\"p0\":1}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":0,\"p0\":2}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":0,\"p0\":3}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":0,\"p0\":4}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":0,\"p0\":5}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":0,\"p0\":6}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":0,\"p0\":7}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":0,\"p0\":8}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":0,\"p0\":9}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":313,\"msg\":\"prompt eval time     =      47.57 ms /    10 tokens (    4.76 ms per token,   210.20 tokens per second)\",\"id_slot\":0,\"id_task\":0,\"t_prompt_processing\":47.573,\"n_prompt_tokens_processed\":10,\"t_token\":4.7573,\"n_tokens_second\":210.20326655876232}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":329,\"msg\":\"generation eval time =       0.00 ms /     1 runs   (    0.00 ms per token, 500000.00 tokens per second)\",\"id_slot\":0,\"id_task\":0,\"t_token_generation\":0.002,\"n_decoded\":1,\"t_token\":0.002,\"n_tokens_second\":500000.0}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":340,\"msg\":\"          total time =      47.58 ms\",\"id_slot\":0,\"id_task\":0,\"t_prompt_processing\":47.573,\"t_token_generation\":0.002,\"t_total\":47.575}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1760,\"msg\":\"slot released\",\"id_slot\":0,\"id_task\":0,\"n_ctx\":512,\"n_past\":10,\"n_system_tokens\":0,\"n_cache_tokens\":9,\"truncated\":false}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1786,\"msg\":\"all slots are idle\"}\n",
      "{\"tid\":\"140135803015168\",\"timestamp\":1713182952,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2868,\"msg\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":50694,\"status\":200,\"method\":\"POST\",\"path\":\"/completion\",\"params\":{}}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"launch_slot_with_task\",\"line\":1037,\"msg\":\"slot is processing task\",\"id_slot\":0,\"id_task\":11}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":11,\"p0\":0}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":11,\"p0\":1}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":11,\"p0\":2}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":11,\"p0\":3}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":11,\"p0\":4}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":11,\"p0\":5}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":11,\"p0\":6}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":11,\"p0\":7}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":11,\"p0\":8}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":11,\"p0\":9}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":313,\"msg\":\"prompt eval time     =      47.15 ms /    10 tokens (    4.71 ms per token,   212.11 tokens per second)\",\"id_slot\":0,\"id_task\":11,\"t_prompt_processing\":47.146,\"n_prompt_tokens_processed\":10,\"t_token\":4.7146,\"n_tokens_second\":212.1070716497688}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":329,\"msg\":\"generation eval time =       0.00 ms /     1 runs   (    0.00 ms per token, 500000.00 tokens per second)\",\"id_slot\":0,\"id_task\":11,\"t_token_generation\":0.002,\"n_decoded\":1,\"t_token\":0.002,\"n_tokens_second\":500000.0}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":340,\"msg\":\"          total time =      47.15 ms\",\"id_slot\":0,\"id_task\":11,\"t_prompt_processing\":47.146,\"t_token_generation\":0.002,\"t_total\":47.148}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1760,\"msg\":\"slot released\",\"id_slot\":0,\"id_task\":11,\"n_ctx\":512,\"n_past\":10,\"n_system_tokens\":0,\"n_cache_tokens\":9,\"truncated\":false}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1786,\"msg\":\"all slots are idle\"}\n",
      "{\"tid\":\"140135803015168\",\"timestamp\":1713183009,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2868,\"msg\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":50212,\"status\":200,\"method\":\"POST\",\"path\":\"/completion\",\"params\":{}}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"WARN\",\"function\":\"launch_slot_with_task\",\"line\":884,\"msg\":\"Max tokens to predict exceeds server configuration\",\"params.n_predict\":1000,\"slot.n_predict\":128}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"launch_slot_with_task\",\"line\":1037,\"msg\":\"slot is processing task\",\"id_slot\":0,\"id_task\":22}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":22,\"p0\":0}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":22,\"p0\":1}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":22,\"p0\":2}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":22,\"p0\":3}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":22,\"p0\":4}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":22,\"p0\":5}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":22,\"p0\":6}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":22,\"p0\":7}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":22,\"p0\":8}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":22,\"p0\":9}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":22,\"p0\":10}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":22,\"p0\":11}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":22,\"p0\":12}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":22,\"p0\":13}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":313,\"msg\":\"prompt eval time     =      65.94 ms /    14 tokens (    4.71 ms per token,   212.33 tokens per second)\",\"id_slot\":0,\"id_task\":22,\"t_prompt_processing\":65.936,\"n_prompt_tokens_processed\":14,\"t_token\":4.709714285714286,\"n_tokens_second\":212.32710507158455}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":329,\"msg\":\"generation eval time =     634.64 ms /   103 runs   (    6.16 ms per token,   162.30 tokens per second)\",\"id_slot\":0,\"id_task\":22,\"t_token_generation\":634.64,\"n_decoded\":103,\"t_token\":6.161553398058253,\"n_tokens_second\":162.29673515693938}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":340,\"msg\":\"          total time =     700.58 ms\",\"id_slot\":0,\"id_task\":22,\"t_prompt_processing\":65.936,\"t_token_generation\":634.64,\"t_total\":700.576}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1760,\"msg\":\"slot released\",\"id_slot\":0,\"id_task\":22,\"n_ctx\":512,\"n_past\":116,\"n_system_tokens\":0,\"n_cache_tokens\":13,\"truncated\":false}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1786,\"msg\":\"all slots are idle\"}\n",
      "{\"tid\":\"140135803015168\",\"timestamp\":1713183021,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2868,\"msg\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":50616,\"status\":200,\"method\":\"POST\",\"path\":\"/completion\",\"params\":{}}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183056,\"level\":\"INFO\",\"function\":\"launch_slot_with_task\",\"line\":1037,\"msg\":\"slot is processing task\",\"id_slot\":0,\"id_task\":139}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183056,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":139,\"p0\":0}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183056,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":139,\"p0\":1}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183056,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":139,\"p0\":2}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183056,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":139,\"p0\":3}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183056,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":139,\"p0\":4}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183056,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":139,\"p0\":5}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183057,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":139,\"p0\":6}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183057,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":139,\"p0\":7}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183057,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":139,\"p0\":8}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183057,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":139,\"p0\":9}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183057,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":313,\"msg\":\"prompt eval time     =      47.14 ms /    10 tokens (    4.71 ms per token,   212.12 tokens per second)\",\"id_slot\":0,\"id_task\":139,\"t_prompt_processing\":47.144,\"n_prompt_tokens_processed\":10,\"t_token\":4.7143999999999995,\"n_tokens_second\":212.11606991345664}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183057,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":329,\"msg\":\"generation eval time =       0.00 ms /     1 runs   (    0.00 ms per token, 500000.00 tokens per second)\",\"id_slot\":0,\"id_task\":139,\"t_token_generation\":0.002,\"n_decoded\":1,\"t_token\":0.002,\"n_tokens_second\":500000.0}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183057,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":340,\"msg\":\"          total time =      47.15 ms\",\"id_slot\":0,\"id_task\":139,\"t_prompt_processing\":47.144,\"t_token_generation\":0.002,\"t_total\":47.146}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183057,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1760,\"msg\":\"slot released\",\"id_slot\":0,\"id_task\":139,\"n_ctx\":512,\"n_past\":10,\"n_system_tokens\":0,\"n_cache_tokens\":9,\"truncated\":false}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183057,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1786,\"msg\":\"all slots are idle\"}\n",
      "{\"tid\":\"140135803015168\",\"timestamp\":1713183057,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2868,\"msg\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":50076,\"status\":200,\"method\":\"POST\",\"path\":\"/completion\",\"params\":{}}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183075,\"level\":\"INFO\",\"function\":\"launch_slot_with_task\",\"line\":1037,\"msg\":\"slot is processing task\",\"id_slot\":0,\"id_task\":150}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183075,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":150,\"p0\":0}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183075,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":150,\"p0\":1}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183075,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":150,\"p0\":2}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183075,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":150,\"p0\":3}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183075,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":150,\"p0\":4}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183075,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":150,\"p0\":5}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183075,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":150,\"p0\":6}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183075,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":150,\"p0\":7}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183075,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":150,\"p0\":8}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183075,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":150,\"p0\":9}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183075,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":150,\"p0\":10}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183075,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":150,\"p0\":11}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183075,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":150,\"p0\":12}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183076,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":313,\"msg\":\"prompt eval time     =      61.19 ms /    13 tokens (    4.71 ms per token,   212.47 tokens per second)\",\"id_slot\":0,\"id_task\":150,\"t_prompt_processing\":61.186,\"n_prompt_tokens_processed\":13,\"t_token\":4.706615384615384,\"n_tokens_second\":212.46690419376984}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183076,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":329,\"msg\":\"generation eval time =     799.20 ms /   128 runs   (    6.24 ms per token,   160.16 tokens per second)\",\"id_slot\":0,\"id_task\":150,\"t_token_generation\":799.197,\"n_decoded\":128,\"t_token\":6.2437265625,\"n_tokens_second\":160.16076136421933}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183076,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":340,\"msg\":\"          total time =     860.38 ms\",\"id_slot\":0,\"id_task\":150,\"t_prompt_processing\":61.186,\"t_token_generation\":799.197,\"t_total\":860.383}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183076,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1760,\"msg\":\"slot released\",\"id_slot\":0,\"id_task\":150,\"n_ctx\":512,\"n_past\":140,\"n_system_tokens\":0,\"n_cache_tokens\":12,\"truncated\":false}\n",
      "{\"tid\":\"140136584015872\",\"timestamp\":1713183076,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1786,\"msg\":\"all slots are idle\"}\n",
      "{\"tid\":\"140135803015168\",\"timestamp\":1713183076,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2868,\"msg\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":50876,\"status\":200,\"method\":\"POST\",\"path\":\"/completion\",\"params\":{}}\n"
     ]
    }
   ],
   "source": [
    "!./llama.cpp/server --threads 1 --threads-batch 1 --threads-http 1 -m {qtype} -n 128 --batch-size 1 -ngl 23 --main-gpu 0 --port 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The server is up and running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it with curl \n",
    "Open the terminal and send your request like the following:\n",
    "\n",
    "curl --request POST \\\n",
    "    --url http://localhost:8080/completion \\\n",
    "    --header \"Content-Type: application/json\" \\\n",
    "    --data '{\"prompt\": \"Building a website can be done in 10 simple steps:\",\"n_predict\": 128}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Server in background and make request from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import os\n",
    "\n",
    "def run_server(qtype):\n",
    "    os.system(f\"./llama.cpp/server --threads 1 --threads-batch 1 --threads-http 1 -m {qtype} -n 1500 --batch-size 1 -ngl 23 --main-gpu 0 --port 8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184300,\"level\":\"INFO\",\"function\":\"main\",\"line\":2921,\"msg\":\"build info\",\"build\":2675,\"commit\":\"17e98d4c\"}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184300,\"level\":\"INFO\",\"function\":\"main\",\"line\":2926,\"msg\":\"system info\",\"n_threads\":1,\"n_threads_batch\":1,\"total_threads\":8,\"system_info\":\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /teamspace/studios/this_studio/pratical-llms/TinyLlama-1.1B-Chat-v1.0/tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type q5_K:  135 tensors\n",
      "llama_model_loader: - type q6_K:   21 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_layer          = 22\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 5632\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 1.10 B\n",
      "llm_load_print_meta: model size       = 745.11 MiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors: offloading 22 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 23/23 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    42.97 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =   702.14 MiB\n",
      "......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 1\n",
      "llama_new_context_with_model: n_ubatch   = 1\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    11.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.24 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =     0.13 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     0.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 710\n",
      "llama_new_context_with_model: graph splits = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184301,\"level\":\"INFO\",\"function\":\"init\",\"line\":708,\"msg\":\"initializing slots\",\"n_slots\":1}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184301,\"level\":\"INFO\",\"function\":\"init\",\"line\":717,\"msg\":\"new slot\",\"id_slot\":0,\"n_ctx_slot\":512}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184301,\"level\":\"INFO\",\"function\":\"main\",\"line\":3021,\"msg\":\"model loaded\"}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184301,\"level\":\"INFO\",\"function\":\"main\",\"line\":3043,\"msg\":\"chat template\",\"chat_example\":\"<|system|>\\nYou are a helpful assistant<|endoftext|>\\n<|user|>\\nHello<|endoftext|>\\n<|assistant|>\\nHi there<|endoftext|>\\n<|user|>\\nHow are you?<|endoftext|>\\n<|assistant|>\\n\",\"built_in\":true}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184301,\"level\":\"INFO\",\"function\":\"main\",\"line\":3774,\"msg\":\"HTTP server listening\",\"n_threads_http\":\"1\",\"port\":\"8080\",\"hostname\":\"127.0.0.1\"}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184301,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1786,\"msg\":\"all slots are idle\"}\n"
     ]
    }
   ],
   "source": [
    "server_thread = threading.Thread(target=run_server,args=(qtype,))\n",
    "server_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting httpx\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: anyio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx) (4.3.0)\n",
      "Requirement already satisfied: certifi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx) (3.6)\n",
      "Requirement already satisfied: sniffio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpcore==1.*->httpx) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio->httpx) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio->httpx) (4.10.0)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: httpcore, httpx\n",
      "Successfully installed httpcore-1.0.5 httpx-0.27.0\n"
     ]
    }
   ],
   "source": [
    "!pip install httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def make_request(prompt,num_tokens=500):\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        # Define your JSON data\n",
    "        json_data = {\"prompt\": f\"{prompt}\",\"n_predict\": num_tokens}\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        # Make a POST request with JSON body\n",
    "        response = await client.post('http://localhost:8080/completion', json=json_data,headers=headers)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184360,\"level\":\"INFO\",\"function\":\"launch_slot_with_task\",\"line\":1037,\"msg\":\"slot is processing task\",\"id_slot\":0,\"id_task\":969}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184360,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":969,\"p0\":0}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184360,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":969,\"p0\":1}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184360,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":969,\"p0\":2}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184360,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":969,\"p0\":3}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184360,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":969,\"p0\":4}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184360,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":969,\"p0\":5}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184360,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":969,\"p0\":6}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184360,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":969,\"p0\":7}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184360,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":969,\"p0\":8}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184360,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2066,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":969,\"p0\":9}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184362,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":313,\"msg\":\"prompt eval time     =      47.22 ms /    10 tokens (    4.72 ms per token,   211.79 tokens per second)\",\"id_slot\":0,\"id_task\":969,\"t_prompt_processing\":47.216,\"n_prompt_tokens_processed\":10,\"t_token\":4.7216000000000005,\"n_tokens_second\":211.79261267366994}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184362,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":329,\"msg\":\"generation eval time =    2190.14 ms /   334 runs   (    6.56 ms per token,   152.50 tokens per second)\",\"id_slot\":0,\"id_task\":969,\"t_token_generation\":2190.145,\"n_decoded\":334,\"t_token\":6.557320359281437,\"n_tokens_second\":152.50131840585897}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184362,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":340,\"msg\":\"          total time =    2237.36 ms\",\"id_slot\":0,\"id_task\":969,\"t_prompt_processing\":47.216,\"t_token_generation\":2190.145,\"t_total\":2237.361}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184362,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1760,\"msg\":\"slot released\",\"id_slot\":0,\"id_task\":969,\"n_ctx\":512,\"n_past\":343,\"n_system_tokens\":0,\"n_cache_tokens\":9,\"truncated\":false}\n",
      "{\"tid\":\"140261659508736\",\"timestamp\":1713184362,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1786,\"msg\":\"all slots are idle\"}\n",
      "{\"tid\":\"140261309014016\",\"timestamp\":1713184362,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2868,\"msg\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":50242,\"status\":200,\"method\":\"POST\",\"path\":\"/completion\",\"params\":{}}\n"
     ]
    }
   ],
   "source": [
    "output = await make_request(\"Here a very large poem about Naples:\",1000)\n",
    "json_output = output.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '\\nNaples is a great city,\\nA great city, a city of song,\\nA city of poems, a city of poets,\\nA city of song,\\nWhere the wind howls,\\nWhere the waves crash,\\nWhere the sun sets,\\nWhere the moon rises,\\nWhere the sun and the moon meet,\\nWhere the stars come out\\nTo play their magic on the sea,\\nWhere the world sings,\\nWhere the world is free,\\nWhere the world is not.\\n\\nThe sun and the moon,\\nIn Naples, are the same,\\nFor they are both the gods,\\nThe gods of the sea,\\nOf love, of power,\\nOf the endless dance,\\nOf the eternal beauty,\\nOf the eternal youth,\\nOf the eternal life.\\n\\nAnd as the sun sets,\\nIn Naples, it is the same,\\nAs the moon rises,\\nIn Naples, it is the same,\\nFor they are both the same,\\nFor they are both the same,\\nFor they are both the same,\\nIn Naples,\\nIn Naples,\\nIn Naples,\\nIn Naples.\\n\\nAnd in Naples,\\nThere is a song,\\nA song that goes on and on,\\nFor it is the song of the sea,\\nOf love, of power,\\nOf the endless dance,\\nOf the eternal beauty,\\nOf the eternal youth,\\nOf the eternal life,\\nOf Naples, Naples, Naples.',\n",
       " 'id_slot': 0,\n",
       " 'stop': True,\n",
       " 'model': '/teamspace/studios/this_studio/pratical-llms/TinyLlama-1.1B-Chat-v1.0/tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf',\n",
       " 'tokens_predicted': 334,\n",
       " 'tokens_evaluated': 10,\n",
       " 'generation_settings': {'n_ctx': 512,\n",
       "  'n_predict': 1500,\n",
       "  'model': '/teamspace/studios/this_studio/pratical-llms/TinyLlama-1.1B-Chat-v1.0/tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf',\n",
       "  'seed': 4294967295,\n",
       "  'temperature': 0.800000011920929,\n",
       "  'dynatemp_range': 0.0,\n",
       "  'dynatemp_exponent': 1.0,\n",
       "  'top_k': 40,\n",
       "  'top_p': 0.949999988079071,\n",
       "  'min_p': 0.05000000074505806,\n",
       "  'tfs_z': 1.0,\n",
       "  'typical_p': 1.0,\n",
       "  'repeat_last_n': 64,\n",
       "  'repeat_penalty': 1.0,\n",
       "  'presence_penalty': 0.0,\n",
       "  'frequency_penalty': 0.0,\n",
       "  'penalty_prompt_tokens': [],\n",
       "  'use_penalty_prompt_tokens': False,\n",
       "  'mirostat': 0,\n",
       "  'mirostat_tau': 5.0,\n",
       "  'mirostat_eta': 0.10000000149011612,\n",
       "  'penalize_nl': False,\n",
       "  'stop': [],\n",
       "  'n_keep': 0,\n",
       "  'n_discard': 0,\n",
       "  'ignore_eos': False,\n",
       "  'stream': False,\n",
       "  'logit_bias': [],\n",
       "  'n_probs': 0,\n",
       "  'min_keep': 0,\n",
       "  'grammar': '',\n",
       "  'samplers': ['top_k',\n",
       "   'tfs_z',\n",
       "   'typical_p',\n",
       "   'top_p',\n",
       "   'min_p',\n",
       "   'temperature']},\n",
       " 'prompt': 'Here a very large poem about Naples:',\n",
       " 'truncated': False,\n",
       " 'stopped_eos': True,\n",
       " 'stopped_word': False,\n",
       " 'stopped_limit': False,\n",
       " 'stopping_word': '',\n",
       " 'tokens_cached': 343,\n",
       " 'timings': {'prompt_n': 10,\n",
       "  'prompt_ms': 47.216,\n",
       "  'prompt_per_token_ms': 4.7216000000000005,\n",
       "  'prompt_per_second': 211.79261267366994,\n",
       "  'predicted_n': 334,\n",
       "  'predicted_ms': 2190.145,\n",
       "  'predicted_per_token_ms': 6.557320359281437,\n",
       "  'predicted_per_second': 152.50131840585897}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 152 token/sec on a T4, not bad. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
