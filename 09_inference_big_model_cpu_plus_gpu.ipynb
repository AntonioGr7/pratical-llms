{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a very large LLM on a consumer Hardware GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:       32387632     2673296     2707528        1236    27006808    29235828\n",
      "Swap:             0           0           0\n",
      "MemTotal:       32387632 kB\n",
      "MemFree:         2706968 kB\n",
      "Cached:         26426084 kB\n",
      "SwapCached:            0 kB\n"
     ]
    }
   ],
   "source": [
    "!free && sync\n",
    "!egrep \"MemTotal|MemFree|Cached\" /proc/meminfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory:31628.546875 MB\n",
      "Available Memory:27564.16015625 MB\n",
      "Memory Usage: 12.9%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import time\n",
    "\n",
    "def memory():\n",
    "    memory_usage = psutil.virtual_memory()\n",
    "    print(f\"Total Memory:{psutil.virtual_memory().total/(1024 * 1024)} MB\")\n",
    "    print(f\"Available Memory:{psutil.virtual_memory().available/(1024 * 1024)} MB\")\n",
    "    print(f\"Memory Usage: {memory_usage.percent}%\")\n",
    "memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 16 13:33:46 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   22C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this machine I have a T4 with 15GB of VRAM and 32 GB of RAM (25 seems to be usable) for a total of 47GB of available memory.\n",
    "### Let's choose something from the Chatbot Arena that fit in this memory and squeeze it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like Qwen/Qwen1.5-32B-Chat that seems to be pretty good for his size. Someone gently quantize it to GGUF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen1.5-32B-Chat-GGUF\"\n",
    "base_model = \"Qwen/Qwen1.5-32B-Chat\" ## The model from which this one was quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I know how much memory this model requires in its quantized versions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def calc_model_size(parameters: int, quant: float) -> int:\n",
    "    return parameters * quant // 8\n",
    "\n",
    "def calc_compute_buffer_size(model_config, context: int) -> float:\n",
    "    return (\n",
    "        (context / 1024 * 2 + 0.75) * model_config[\"num_attention_heads\"] * 1024 * 1024\n",
    "    )\n",
    "\n",
    "def calc_input_buffer_size(model_config, context: int) -> float:\n",
    "    return 4096 + 2048 * model_config[\"hidden_size\"] + context * 4 + context * 2048\n",
    "\n",
    "def calc_context_size(model_config, context: int) -> float:\n",
    "    n_gqa = model_config[\"num_attention_heads\"] / model_config[\"num_key_value_heads\"]\n",
    "    n_embd_gqa = model_config[\"hidden_size\"] / n_gqa\n",
    "    n_elements = n_embd_gqa * (model_config[\"num_hidden_layers\"] * context)\n",
    "    return 2 * n_elements * 2\n",
    "\n",
    "def get_model_config(model_config,model_size):\n",
    "    config = model_config\n",
    "    config[\"parameters\"] = model_size / 2\n",
    "\n",
    "def calc(model_config, model_size,context, quant_size):\n",
    "    quant_bpw = 0\n",
    "    model_config['parameters'] = model_size / 2\n",
    "    try:\n",
    "        quant_bpw = float(quant_size)\n",
    "    except:\n",
    "        quant_bpw = quants[quant_size]\n",
    "\n",
    "    model_size = round(\n",
    "        calc_model_size(model_config[\"parameters\"], quant_bpw) / 1000 / 1000 / 1000, 2\n",
    "    )\n",
    "    context_size = round(\n",
    "        (\n",
    "            calc_input_buffer_size(model_config, context)\n",
    "            + calc_context_size(model_config, context)\n",
    "            + calc_compute_buffer_size(model_config, context)\n",
    "        )\n",
    "        / 1000\n",
    "        / 1000\n",
    "        / 1000,\n",
    "        2,\n",
    "    )\n",
    "\n",
    "    return model_size, context_size, round(model_size + context_size, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the config.json taken from the official HF repo of Qwen/Qwen1.5-32B-Chat and model.safetensors.index.json or pytorch_model.bin.index.json.\n",
    "Basically we need the \"total_size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = json.loads(\"\"\"{\n",
    "  \"architectures\": [\n",
    "    \"Qwen2ForCausalLM\"\n",
    "  ],\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 151643,\n",
    "  \"eos_token_id\": 151645,\n",
    "  \"hidden_act\": \"silu\",\n",
    "  \"hidden_size\": 5120,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 27392,\n",
    "  \"max_position_embeddings\": 32768,\n",
    "  \"max_window_layers\": 35,\n",
    "  \"model_type\": \"qwen2\",\n",
    "  \"num_attention_heads\": 40,\n",
    "  \"num_hidden_layers\": 64,\n",
    "  \"num_key_value_heads\": 8,\n",
    "  \"rms_norm_eps\": 1e-06,\n",
    "  \"rope_theta\": 1000000.0,\n",
    "  \"sliding_window\": 32768,\n",
    "  \"tie_word_embeddings\": false,\n",
    "  \"torch_dtype\": \"bfloat16\",\n",
    "  \"transformers_version\": \"4.37.2\",\n",
    "  \"use_cache\": true,\n",
    "  \"use_sliding_window\": false,\n",
    "  \"vocab_size\": 152064\n",
    "}\"\"\")\n",
    "\n",
    "metadata = json.loads(\"\"\"{\n",
    "  \"metadata\": {\n",
    "    \"total_size\": 65024436224\n",
    "  }}\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example let's assume we want a Q5_K_M version. The map below tells us how many bits are used per parameter in that type of quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "quants = {\n",
    "    \"Q2_K\": 3.35,\n",
    "    \"Q3_K_S\": 3.5,\n",
    "    \"Q3_K_M\": 3.91,\n",
    "    \"Q3_K_L\": 4.27,\n",
    "    \"Q4_0\": 4.55,\n",
    "    \"Q4_K_S\": 4.58,\n",
    "    \"Q4_K_M\": 4.85,\n",
    "    \"Q5_0\": 5.54,\n",
    "    \"Q5_K_S\": 5.54,\n",
    "    \"Q5_K_M\": 5.69,\n",
    "    \"Q6_K\": 6.59,\n",
    "    \"Q8_0\": 8.5,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size GB:23.12\n",
      "Context Size GB:5.58\n",
      "Total Size GB:28.7\n"
     ]
    }
   ],
   "source": [
    "context_size = 16000\n",
    "\n",
    "model_size_gb, context_size_gb, total_size_gb = calc(model_config=config,model_size=metadata['metadata']['total_size'],context=context_size,quant_size=quants['Q5_K_M'])\n",
    "print(f\"Model Size GB:{model_size_gb}\")\n",
    "print(f\"Context Size GB:{context_size_gb}\")\n",
    "print(f\"Total Size GB:{total_size_gb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that we need 29GB of RAM to run Q5_K_M version with 16k context lenght. Let's verify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.getcwd()+\"/pratical-llms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-16 13:05:08--  https://huggingface.co/Qwen/Qwen1.5-32B-Chat-GGUF/resolve/main/qwen1_5-32b-chat-q5_k_m.gguf?download=true\n",
      "Resolving huggingface.co (huggingface.co)... 18.154.227.67, 18.154.227.87, 18.154.227.7, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.154.227.67|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/96/88/96886dc2b365648803a6c4c31290ebe172329dddf1699fca4b5fd6f0a9868aca/9f7f066e1ef9453f0d38f08af0795af6022caf81bb1abad38893d79b3c373b4c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27qwen1_5-32b-chat-q5_k_m.gguf%3B+filename%3D%22qwen1_5-32b-chat-q5_k_m.gguf%22%3B&Expires=1713531908&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzUzMTkwOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzk2Lzg4Lzk2ODg2ZGMyYjM2NTY0ODgwM2E2YzRjMzEyOTBlYmUxNzIzMjlkZGRmMTY5OWZjYTRiNWZkNmYwYTk4NjhhY2EvOWY3ZjA2NmUxZWY5NDUzZjBkMzhmMDhhZjA3OTVhZjYwMjJjYWY4MWJiMWFiYWQzODg5M2Q3OWIzYzM3M2I0Yz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=JL5tDYIL8nBVhwEpL4ZaeFjEh0DEZOosWPXpW3xwd-qQ2bT40ipmz0AFfp42FeAoEHWpZyiTFVsEfU%7EcSPvcbJH%7EsS5sow%7E0UfzyamXNH25putxDAdYglF5DN5XwPToxMdPLG-h1l5zhqcCUKW8LwIsqnx27h14m48mFWKfnBzEMfe501aa8-1g43oe639sf3dLG1i7MzsXujGy6XAhUuXgM2EI2iI30VKikeWoKeMKqACF0V4aZWAVPJtGsURr1biq2pnwp2qzQTIAMneniD4MrmjSavfywrHCZ8k-5nzbk7lonUSUqcen6c9dO2i9zdUdLdzya8n180JQG96ND7Q__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
      "--2024-04-16 13:05:08--  https://cdn-lfs-us-1.huggingface.co/repos/96/88/96886dc2b365648803a6c4c31290ebe172329dddf1699fca4b5fd6f0a9868aca/9f7f066e1ef9453f0d38f08af0795af6022caf81bb1abad38893d79b3c373b4c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27qwen1_5-32b-chat-q5_k_m.gguf%3B+filename%3D%22qwen1_5-32b-chat-q5_k_m.gguf%22%3B&Expires=1713531908&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzUzMTkwOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzk2Lzg4Lzk2ODg2ZGMyYjM2NTY0ODgwM2E2YzRjMzEyOTBlYmUxNzIzMjlkZGRmMTY5OWZjYTRiNWZkNmYwYTk4NjhhY2EvOWY3ZjA2NmUxZWY5NDUzZjBkMzhmMDhhZjA3OTVhZjYwMjJjYWY4MWJiMWFiYWQzODg5M2Q3OWIzYzM3M2I0Yz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=JL5tDYIL8nBVhwEpL4ZaeFjEh0DEZOosWPXpW3xwd-qQ2bT40ipmz0AFfp42FeAoEHWpZyiTFVsEfU%7EcSPvcbJH%7EsS5sow%7E0UfzyamXNH25putxDAdYglF5DN5XwPToxMdPLG-h1l5zhqcCUKW8LwIsqnx27h14m48mFWKfnBzEMfe501aa8-1g43oe639sf3dLG1i7MzsXujGy6XAhUuXgM2EI2iI30VKikeWoKeMKqACF0V4aZWAVPJtGsURr1biq2pnwp2qzQTIAMneniD4MrmjSavfywrHCZ8k-5nzbk7lonUSUqcen6c9dO2i9zdUdLdzya8n180JQG96ND7Q__&Key-Pair-Id=KCD77M1F0VK2B\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 18.160.46.93, 18.160.46.36, 18.160.46.26, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|18.160.46.93|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23083568832 (21G) [binary/octet-stream]\n",
      "Saving to: ‘qwen1_5-32b-chat-q5_k_m.gguf?download=true’\n",
      "\n",
      "qwen1_5-32b-chat-q5 100%[===================>]  21.50G  95.1MB/s    in 3m 31s  \n",
      "\n",
      "2024-04-16 13:08:39 (105 MB/s) - ‘qwen1_5-32b-chat-q5_k_m.gguf?download=true’ saved [23083568832/23083568832]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/Qwen/Qwen1.5-32B-Chat-GGUF/resolve/main/qwen1_5-32b-chat-q5_k_m.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/pratical-llms/qwen1_5-32b-chat-q5_k_m.gguf'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.getcwd()+\"/qwen1_5-32b-chat-q5_k_m.gguf\"\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGive me a short introduction to large language model.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-32B-Chat\")\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 2675 (17e98d4c)\n",
      "main: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1713275233\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 771 tensors from /teamspace/studios/this_studio/pratical-llms/qwen1_5-32b-chat-q5_k_m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = Qwen2-beta-32B-Chat-AWQ-fp16\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 64\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 27392\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  321 tensors\n",
      "llama_model_loader: - type q5_K:  385 tensors\n",
      "llama_model_loader: - type q6_K:   65 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 421/152064 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 152064\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 64\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 5\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 27392\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 32.51 B\n",
      "llm_load_print_meta: model size       = 21.49 GiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen2-beta-32B-Chat-AWQ-fp16\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.59 MiB\n",
      "llm_load_tensors: offloading 35 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 35/65 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 22008.51 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 11414.43 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =    58.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    70.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   916.08 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    11.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2246\n",
      "llama_new_context_with_model: graph splits = 410\n",
      "\n",
      "system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 2048, n_predict = 500, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33msystem\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Give me a short introduction to large language model.\n",
      "assistant\n",
      "\u001b[0mLarge language models are artificial intelligence systems designed to process and generate human-like language at an unprecedented scale. These models are built using deep learning techniques, specifically deep neural networks, and are trained on massive amounts of text data, often spanning billions or trillions of words. By learning patterns and relationships within this data, they acquire the ability to understand and generate text across a wide range of topics and styles.\n",
      "\n",
      "The key characteristic of large language models is their size, which refers to the number of parameters they have learned during training. This vast parameter count enables them to capture intricate language structures and nuances, allowing them to perform a variety of natural language processing tasks, such as language translation, question-answering, text summarization, sentiment analysis, and even creative writing.\n",
      "\n",
      "Prominent examples of large language models include GPT (Generative Pre-trained Transformer), created by OpenAI, and BERT (Bidirectional Encoder Representations from Transformers), developed by Google. These models have pushed the boundaries of what is possible with AI-generated language, achieving remarkable results in terms of fluency, coherence, and context awareness. However, they also present challenges, such as the potential for bias in their training data and the ethical considerations around their use. [end of text]\n",
      "\n",
      "llama_print_timings:        load time =   59691.71 ms\n",
      "llama_print_timings:      sample time =      34.31 ms /   247 runs   (    0.14 ms per token,  7198.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12905.83 ms /    29 tokens (  445.03 ms per token,     2.25 tokens per second)\n",
      "llama_print_timings:        eval time =  157831.47 ms /   246 runs   (  641.59 ms per token,     1.56 tokens per second)\n",
      "llama_print_timings:       total time =  171284.84 ms /   275 tokens\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./llama.cpp/main -m {model_path} -n 500 --color -ngl 35 -p \"{prompt}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great it works!\n",
    "Here the output \n",
    "\n",
    "assistant\n",
    "\n",
    "Large language models are artificial intelligence systems designed to process and generate human-like language at an unprecedented scale. These models are built using deep learning techniques, specifically deep neural networks, and are trained on massive amounts of text data, often spanning billions or trillions of words. By learning patterns and relationships within this data, they acquire the ability to understand and generate text across a wide range of topics and styles.\n",
    "\n",
    "The key characteristic of large language models is their size, which refers to the number of parameters they have learned during training. This vast parameter count enables them to capture intricate language structures and nuances, allowing them to perform a variety of natural language processing tasks, such as language translation, question-answering, text summarization, sentiment analysis, and even creative writing.\n",
    "\n",
    "Prominent examples of large language models include GPT (Generative Pre-trained Transformer), created by OpenAI, and BERT (Bidirectional Encoder Representations from Transformers), developed by Google. These models have pushed the boundaries of what is possible with AI-generated language, achieving remarkable results in terms of fluency, coherence, and context awareness. However, they also present challenges, such as the potential for bias in their training data and the ethical considerations around their use. [end of text]\n",
    "\n",
    "llama_print_timings:        load time =   59691.71 ms\n",
    "llama_print_timings:      sample time =      34.31 ms /   247 runs   (    0.14 ms per token,  7198.23 tokens per second)\n",
    "llama_print_timings: prompt eval time =   12905.83 ms /    29 tokens (  445.03 ms per token,     2.25 tokens per second)\n",
    "llama_print_timings:        eval time =  157831.47 ms /   246 runs   (  641.59 ms per token,     1.56 tokens per second)\n",
    "llama_print_timings:       total time =  171284.84 ms /   275 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.56 tokens/s is not that fast, but it works.\n",
    "Let's analyze the RAM and the VRAM better. The model have 65 layers. Let's offload until it runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tid\":\"139882769641472\",\"timestamp\":1713276081,\"level\":\"INFO\",\"function\":\"main\",\"line\":2921,\"msg\":\"build info\",\"build\":2675,\"commit\":\"17e98d4c\"}\n",
      "{\"tid\":\"139882769641472\",\"timestamp\":1713276081,\"level\":\"INFO\",\"function\":\"main\",\"line\":2926,\"msg\":\"system info\",\"n_threads\":1,\"n_threads_batch\":1,\"total_threads\":8,\"system_info\":\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \"}\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 771 tensors from /teamspace/studios/this_studio/pratical-llms/qwen1_5-32b-chat-q5_k_m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = Qwen2-beta-32B-Chat-AWQ-fp16\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 64\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 27392\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  321 tensors\n",
      "llama_model_loader: - type q5_K:  385 tensors\n",
      "llama_model_loader: - type q6_K:   65 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 421/152064 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 152064\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 64\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 5\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 27392\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 32.51 B\n",
      "llm_load_print_meta: model size       = 21.49 GiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen2-beta-32B-Chat-AWQ-fp16\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.59 MiB\n",
      "llm_load_tensors: offloading 45 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 45/65 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 22008.51 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 14659.90 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 1\n",
      "llama_new_context_with_model: n_ubatch   = 1\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =    38.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    90.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     1.16 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =     0.27 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     0.60 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2246\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "{\"tid\":\"139882769641472\",\"timestamp\":1713276091,\"level\":\"INFO\",\"function\":\"init\",\"line\":708,\"msg\":\"initializing slots\",\"n_slots\":1}\n",
      "{\"tid\":\"139882769641472\",\"timestamp\":1713276091,\"level\":\"INFO\",\"function\":\"init\",\"line\":717,\"msg\":\"new slot\",\"id_slot\":0,\"n_ctx_slot\":512}\n",
      "{\"tid\":\"139882769641472\",\"timestamp\":1713276091,\"level\":\"INFO\",\"function\":\"main\",\"line\":3021,\"msg\":\"model loaded\"}\n",
      "{\"tid\":\"139882769641472\",\"timestamp\":1713276091,\"level\":\"INFO\",\"function\":\"main\",\"line\":3043,\"msg\":\"chat template\",\"chat_example\":\"<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\nHello<|im_end|>\\n<|im_start|>assistant\\nHi there<|im_end|>\\n<|im_start|>user\\nHow are you?<|im_end|>\\n<|im_start|>assistant\\n\",\"built_in\":true}\n",
      "{\"tid\":\"139882769641472\",\"timestamp\":1713276091,\"level\":\"INFO\",\"function\":\"main\",\"line\":3774,\"msg\":\"HTTP server listening\",\"n_threads_http\":\"1\",\"port\":\"8080\",\"hostname\":\"127.0.0.1\"}\n",
      "{\"tid\":\"139882769641472\",\"timestamp\":1713276091,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1786,\"msg\":\"all slots are idle\"}\n"
     ]
    }
   ],
   "source": [
    "!./llama.cpp/server --threads 1 --threads-batch 1 --threads-http 1 -m {model_path} -n 16000 --batch-size 1 -ngl 45 --main-gpu 0 --port 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the following images, we are now using almost 15GB of GPU VRAM and 13GB of Memory for a total of 28GB that seems coherent with what expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vram_gpu.png](images/vram_gpu.png)\n",
    "![mem_before.png](images/before_mem.png)\n",
    "![vram_gpu.png](images/mem_after.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
